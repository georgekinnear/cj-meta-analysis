---
title: 'CJ meta-analysis: factors affecting reliability measures'
author: "George Kinnear"
date: "2022-08-22"
output: github_document
always_allow_html: true
---

```{r setup, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.path='figs-web/04-factors-affecting-reliability/')
knitr::opts_chunk$set(dpi=300,fig.width=5)

# for plotting
theme_set(theme_minimal())
#library(patchwork)

library(knitr)
library(kableExtra)
basic_kable = function(df, ...) {
  df %>% 
    kable(...) %>%
    kable_styling(bootstrap_options = "striped", full_width = F)
}
```

# About the sample

```{r}
cj_sessions <- read_csv("data/00-judging_sessions_summary.csv", show_col_types = FALSE)
reliability_stats <- read_csv("data/01-meta-analysis-data.csv", show_col_types = FALSE)

meta_analysis_data <- cj_sessions %>% 
  left_join(reliability_stats, by = "judging_session")

# avoid having to load all the judgement data, by pre-computing the stats in 02-computation.R and loading the results from the 01-meta-analysis-data.csv above.
# source("00-load-all-judgement-data.R")
```

The judging sessions in the sample span a wide range of number of judgements made:

```{r num-judgements}
meta_analysis_data %>% 
  filter(observed_N_C > 0) %>% 
  ggplot(aes(x = observed_N_C)) +
  geom_histogram(bins = 30) +
  scale_x_continuous(trans='log10', labels = scales::trans_format("log10", scales::math_format(10^.x))) +
  labs(x = "Total number of judgements in the session", y = "Number of sessions in our sample")
```

Similarly, the number of judges in each session varies a lot:

```{r num-judges}
meta_analysis_data %>% 
  filter(observed_N_A > 0) %>% 
  ggplot(aes(x = observed_N_A)) +
  geom_histogram(bins = 30) +
  scale_x_continuous(trans='log10') +
  labs(x = "Number of judges", y = "Number of sessions in our sample")
```

There is also a wide range of numbers of items in the different judging sessions - from a handful to a maximum of `r max(meta_analysis_data$observed_N_R, na.rm = TRUE)`.

```{r num-items}
meta_analysis_data %>% 
  filter(observed_N_R > 0) %>% 
  ggplot(aes(x = observed_N_R)) +
  geom_histogram(bins = 30) +
  scale_x_continuous(trans='log10') +
  labs(x = "Number of items being judged", y = "Number of sessions in our sample")
```

# Proportion of the comparison matrix that is filled

For a judging session with $N$ items, there are $\frac{N(N-1)}{2}$ possible pairs to judge.

Perhaps a factor that influences reliability is how many of these possible pairs are actually considered by judges?

We compute the number of possible pairs in each judging session, and compare that with the actual number of pairs that were seen by judges.

There is a bimodal pattern. Most commonly, all (or nearly all) pairs are judged. But another common case is for only around 10% of possible pairs to be judged.

```{r prop-judged}
meta_analysis_data %>% 
  filter(!is.na(num_pairs)) %>% 
  select(judging_session, num_pairs, num_pairs_judged, prop_pairs_judged) %>% 
  ggplot(aes(x = prop_pairs_judged)) +
  geom_histogram(binwidth = 0.05) +
  labs(x = "Proportion of pairs judged")
```

The proportion of pairs judged does not seem to have a relationship with reliability:

```{r propjudged-vs-splithalves}
meta_analysis_data %>% 
  filter(!is.na(prop_pairs_judged)) %>% 
  ggplot(aes(x = prop_pairs_judged, y = mean_split_corr)) +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_point() +
  geom_smooth(method = "lm", formula = "y ~ x") +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(x = "Proportion of pairs judged", y = "Mean split-halves correlation")
```

# Percentage agreement

What proportion of the decisions agree with the final rank order?

```{r propcorrect}
meta_analysis_data %>% 
  filter(!is.na(num_pairs)) %>% 
  select(judging_session, prop_correct_judgements) %>% 
  ggplot(aes(x = prop_correct_judgements)) +
  geom_histogram(binwidth = 0.05) +
  labs(x = "Proportion of pairs judged in agreement with final rank order")
```

It is generally quite high, but with a few on the low end - here are details of all the ones with less than 70% in agreement:

```{r}
meta_analysis_data %>% 
  filter(!is.na(num_pairs)) %>% 
  select(judging_session, prop_correct_judgements, starts_with("observed"), judge_expertise, adaptivity) %>% 
  filter(prop_correct_judgements < 0.7) %>% 
  arrange(prop_correct_judgements) %>% 
  basic_kable()
```

```{r propcorrect-vs-splithalves}
meta_analysis_data %>% 
  filter(!is.na(prop_correct_judgements)) %>% 
  ggplot(aes(x = prop_correct_judgements, y = mean_split_corr)) +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_point() +
  geom_smooth(method = "lm", formula = "y ~ x") +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(x = "Proportion of pairs judged 'correctly'", y = "Mean split-halves correlation")
```


# Number of comparisons per representation

This is considered important by Verhavert (2019).

But an initial look at our data suggest it's not so closely linked to split-halves:

```{r ncr-splithalves}
meta_analysis_data %>% 
  filter(!is.na(observed_N_C)) %>% 
  mutate(N_CR = observed_N_C / observed_N_R) %>% 
  ggplot(aes(x = N_CR, y = mean_split_corr)) +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_point() +
  geom_smooth(method = "lm", formula = "y ~ x") +
  xlim(c(0,250)) +
  labs(x = "Number of comparisons per representation", y = "Mean split-halves correlation")
```

Same with SSR:

```{r ncr-ssr}
meta_analysis_data %>% 
  filter(!is.na(observed_N_C)) %>% 
  mutate(N_CR = observed_N_C / observed_N_R) %>% 
  ggplot(aes(x = N_CR, y = ssr)) +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_point() +
  geom_smooth(method = "lm", formula = "y ~ x") +
  xlim(c(0,250)) +
  labs(x = "Number of comparisons per representation", y = "SSR")
```

Unlike the Verhavert (2019) sample, we seem to have many judging sessions with low N_CR yet high reliability.

## Adaptivity

All the examples with $N_{CR} > 100$ are not adaptive, so we focus the plot on $N_{CR} < 100$ to see the details better.

Observations:

1. the adaptive ones tend to have lower N_CR and are among the higher split-halves for a given N_CR level.

2. the "unknown" ones look quite similar to the adaptive ones, so chances are they were adaptive too?

```{r ncr-splithalves-adaptivity}
meta_analysis_data %>% 
  filter(!is.na(observed_N_C)) %>% 
  mutate(N_CR = observed_N_C / observed_N_R) %>% 
  ggplot(aes(x = N_CR, y = mean_split_corr, colour = adaptivity, shape = adaptivity)) +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_point() +
  #geom_smooth(method = "lm", formula = "y ~ x") +
  xlim(c(0,100)) +
  labs(x = "Number of comparisons per representation", y = "Mean split-halves correlation")
```

## Judge expertise

Of the judging sessions with $N_{CR} >100$, all but one (Esen2019) have layperson judges.

```{r ncr-splithalves-expertise}
meta_analysis_data %>% 
  filter(!is.na(observed_N_C)) %>% 
  mutate(N_CR = observed_N_C / observed_N_R) %>% 
  ggplot(aes(x = N_CR, y = mean_split_corr, colour = judge_expertise, shape = judge_expertise)) +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_point() +
  #geom_smooth(method = "lm", formula = "y ~ x") +
  scale_shape_manual(values = c(15,16,17,18)) +
  scale_colour_viridis_d(direction = -1) +
  xlim(c(0,100)) +
  labs(x = "Number of comparisons per representation", y = "Mean split-halves correlation")
```

## Number of judges

The number of judges varies over quite a large range, so to colour the points we use $\log_2(N_A)$, so the colours correspond to the nearest power of 2.

```{r ncr-splithalves-judges}
meta_analysis_data %>% 
  filter(!is.na(observed_N_C)) %>% 
  mutate(N_CR = observed_N_C / observed_N_R) %>% 
  ggplot(aes(x = N_CR, y = mean_split_corr, colour = log2(observed_N_A))) +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_point() +
  scale_colour_viridis_c() +
  #xlim(c(0,100)) +
  labs(x = "Number of comparisons per representation", y = "Mean split-halves correlation")
```

Here we zoom in on $N_{CR}<100$:

```{r ncr-splithalves-judges-zoom}
meta_analysis_data %>% 
  filter(!is.na(observed_N_C)) %>% 
  mutate(N_CR = observed_N_C / observed_N_R) %>% 
  filter(N_CR < 100) %>% 
  ggplot(aes(x = N_CR, y = mean_split_corr, colour = log2(observed_N_A))) +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_point() +
  scale_colour_viridis_c() +
  labs(x = "Number of comparisons per representation", y = "Mean split-halves correlation")
```
