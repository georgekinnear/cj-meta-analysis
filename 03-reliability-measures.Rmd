---
title: 'CJ meta-analysis: reliability measures'
author: "George Kinnear"
date: "2022-08-22"
output: github_document
always_allow_html: true
---

```{r setup, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.path='figs-web/03-reliability-measures/')
knitr::opts_chunk$set(dpi=300,fig.width=7)

# for plotting
theme_set(theme_minimal())
#library(patchwork)

library(knitr)
library(kableExtra)
basic_kable = function(df, ...) {
  df %>% 
    kable(...) %>%
    kable_styling(bootstrap_options = "striped", full_width = F)
}
```

# About the sample

```{r}
cj_sessions <- read_csv("data/00-judging_sessions_summary.csv", show_col_types = FALSE)
reliability_stats <- read_csv("data/01-meta-analysis-data.csv", show_col_types = FALSE)

meta_analysis_data <- cj_sessions %>% 
  left_join(reliability_stats, by = "judging_session")
```

```{r}
meta_analysis_data %>% 
  mutate(source = replace_na(source, "literature search")) %>% 
  janitor::tabyl(source) %>% 
  janitor::adorn_pct_formatting(digits = 0) %>% 
  basic_kable()
```


```{r}
meta_analysis_data %>% 
  janitor::tabyl(adaptivity) %>% 
  janitor::adorn_pct_formatting(digits = 0) %>% 
  basic_kable()
```

# SSR vs split-halves

```{r ssr-vs-splithalves}
meta_analysis_data %>% 
  filter(!is.na(ssr)) %>% 
  mutate(pt_label = ifelse(mean_split_corr < 0.5 | ssr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = ssr, y = mean_split_corr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_point() +
  geom_smooth(method = "lm", formula = "y ~ x") +
  ggrepel::geom_text_repel(alpha = 0.3) +
  geom_abline(slope = 1, intercept = 0) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(x = "SSR", y = "Mean split-halves correlation")
```

It is very rare for the split-halves measure to be higher than SSR:

```{r}
meta_analysis_data %>% 
  filter(mean_split_corr > ssr) %>% select(judging_session, starts_with("observed"), mean_split_corr, ssr) %>% 
  basic_kable(digits = 3)
```

# Low mean split-halves reliability

These sessions have split-halves reliability below 0.7.

The `N_CR` column shows the "number of comparisons per representation", i.e. `N_C/N_R`.

```{r}
low_splithalves <- meta_analysis_data %>% 
  filter(mean_split_corr < 0.7) %>% 
  select(judging_session, starts_with("observed"), mean_split_corr, ssr) %>% 
  purrr::set_names(~ str_remove(., "observed_")) %>% 
  mutate(N_CR = N_C/N_R, .after = "N_C") %>% 
  arrange(mean_split_corr)

low_splithalves %>% 
  basic_kable(digits = 3)
```

Omitting the three sessions with N_CR = 400:

```{r low-splithaves}
low_splithalves %>% 
  ggplot(aes(y = mean_split_corr, x = N_CR)) +
  geom_point() +
  xlim(c(0,100))
```

# EloChoice

Here we look at how the EloChoice results compare with the Bradley-Terry ones.

## Reliability

First of all, how does the measure of reliability in EloChoice compare with SSR/split-halves?

There are two reliability measures proposed for EloChoice: see discussion in https://cran.r-project.org/web/packages/EloChoice/vignettes/EloChoice-tutorial.html

We use the weighted version, $R'$, computed based on 1000 iterations.

### SSR vs EloChoice

```{r ssr-vs-elo}
meta_analysis_data %>% 
  filter(!is.na(mean_eloR_weighted)) %>% 
  mutate(pt_label = ifelse(mean_eloR_weighted < 0.5 | ssr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = ssr, y = mean_eloR_weighted, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_point() +
  geom_smooth(method = "lm", formula = "y ~ x") +
  ggrepel::geom_text_repel(alpha = 0.3) +
  geom_abline(slope = 1, intercept = 0) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(x = "SSR", y = "Mean EloChoice R (weighted)")
```

### Split-halves vs EloChoice

```{r splithalves-vs-elo}
meta_analysis_data %>% 
  filter(!is.na(mean_eloR_weighted)) %>% 
  mutate(pt_label = ifelse(mean_eloR_weighted < 0.5 | mean_split_corr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = mean_split_corr, y = mean_eloR_weighted, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_point() +
  geom_smooth(method = "lm", formula = "y ~ x") +
  ggrepel::geom_text_repel(alpha = 0.3) +
  geom_abline(slope = 1, intercept = 0) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(x = "Mean split-halves", y = "Mean EloChoice R (weighted)")
```

## Scores

How do the scores produced by EloChoice compare with those from Bradley-Terry?

Previous work has found high agreement:

* 0.95 and 0.94 in Clark et al. (2018) https://doi.org/10.1371/journal.pone.0190393
* Kendall's tau score of 0.96 for rank orders in Gray et al. (2022) https://doi.org/10.48550/arXiv.2204.01805

```{r elobtmcorr-histogram}
meta_analysis_data %>% 
  filter(!is.na(elo_btm_correlation)) %>% 
  ggplot(aes(x = elo_btm_correlation)) +
  geom_histogram(binwidth = 0.01)
```

```{r}
meta_analysis_data %>% 
  filter(elo_btm_correlation < 0.9) %>% 
  arrange(-elo_btm_correlation) %>% 
  select(elo_btm_correlation, judging_session, starts_with("observed_"), mean_eloR_weighted, mean_split_corr, ssr) %>% 
  basic_kable()
```

The $R'$ measure is quite low for all of those.

Looking at the relationship of $R'$ with the Elo-BTM correlation:

```{r elo-reliability-vs-btmcorr}
meta_analysis_data %>% 
  filter(!is.na(elo_btm_correlation)) %>% 
  ggplot(aes(x = mean_eloR_weighted, y = elo_btm_correlation)) +
  geom_point()
```

It's odd that there are some judging sessions with low $R'$ but high correlation of Elo and BTM scores.

```{r}
meta_analysis_data %>% 
  filter(elo_btm_correlation > 0.95, mean_eloR_weighted < 0.6) %>% 
  arrange(-elo_btm_correlation) %>% 
  select(elo_btm_correlation, judging_session, starts_with("observed_"), mean_eloR_weighted, mean_split_corr, ssr) %>% 
  basic_kable()
```

Those ones all seem to have a small number of representations - so perhaps the $R'$ measure is systematically underestimating reliability in those cases.


