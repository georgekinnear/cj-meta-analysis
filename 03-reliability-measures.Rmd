---
title: 'CJ meta-analysis: reliability measures'
author: "George Kinnear"
date: "2022-08-22"
output: github_document
always_allow_html: true
---

```{r setup, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.path='figs-web/03-reliability-measures/')
knitr::opts_chunk$set(dpi=300,fig.width=7)

# for plotting
theme_set(theme_minimal())
#library(patchwork)

library(knitr)
library(kableExtra)
basic_kable = function(df, ...) {
  df %>% 
    kable(...) %>%
    kable_styling(bootstrap_options = "striped", full_width = F)
}
```

# About the sample

```{r}
cj_sessions <- read_csv("data/00-judging_sessions_summary.csv", show_col_types = FALSE)
reliability_stats <- read_csv("data/01-meta-analysis-data.csv", show_col_types = FALSE)

meta_analysis_data <- cj_sessions %>% 
  left_join(reliability_stats, by = "judging_session")
```

```{r}
meta_analysis_data %>% 
  mutate(source = replace_na(source, "literature search")) %>% 
  janitor::tabyl(source) %>% 
  janitor::adorn_pct_formatting(digits = 0) %>% 
  basic_kable()
```


```{r}
meta_analysis_data %>% 
  janitor::tabyl(adaptivity) %>% 
  janitor::adorn_pct_formatting(digits = 0) %>% 
  basic_kable()
```


```{r include=FALSE}
CperR <- round(cj_sessions$observed_N_C/cj_sessions$observed_N_R,0)
meta_analysis_data$CperR <- CperR
meta_analysis_data_34 <- meta_analysis_data[meta_analysis_data$CperR > 33,]
meta_analysis_data_20 <- meta_analysis_data[meta_analysis_data$CperR > 19,]
meta_analysis_data_10 <- meta_analysis_data[meta_analysis_data$CperR > 9,]
```

# SSR computation

There seems to be a problem with the way `sirt::btm` computes the SSR:

> `mle.rel <- 1 - v2 / v0`
> <https://github.com/alexanderrobitzsch/sirt/blame/d0afec2822740805476055add1ba6b8bd2f04a37/R/btm.R#L265>

The SSR should be nonnegative, but in some cases this formula can give negative results. It tends to be in close agreement with the true SSR for higher values, but can diverge when there is limited judgement data.

Here we see how the values compare in our sample - the `ssr` column is the faulty value from `sirt::btm`, while `ssr_alt` comes from computing the correct value as G^2 / (1+G^2).

```{r}
meta_analysis_data <- meta_analysis_data %>% 
  mutate(ssr_alt = sepG^2 / (1+sepG^2))

ssr_comparison_data <- meta_analysis_data %>% 
  transmute(judging_session, ssr, sepG, ssr_alt, ssr_diff = abs(ssr_alt - ssr)) %>% 
  arrange(-ssr_diff)

ssr_comparison_data %>% basic_kable()
```
```{r}
ssr_comparison_data %>% 
  ggplot(aes(x = ssr_alt, y = ssr)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0) +
  labs(x = "Correct SSR", y = "sirt::btm value for SSR")
```

So for the following analyses, we use the correct value of the SSR instead.

```{r}
meta_analysis_data <- meta_analysis_data %>% 
  # keep both values around just in case
  mutate(ssr_bad = ssr) %>% 
  mutate(ssr_good = ssr_alt) %>% 
  # but pick the correct one for going forward
  mutate(ssr = ssr_good) %>% 
  # tidy up the presentation of the adaptivity field
  mutate(adaptivity = str_replace(adaptivity, "TRUE", "Adaptive") %>% str_replace("FALSE", "Non-adapative"))
```


# SSR vs split-halves

```{r ssr-vs-splithalves}
SSRvSplitHalves <- cor.test(meta_analysis_data$ssr, meta_analysis_data$mean_split_corr)
SSRvSplitHalves_r <- round(SSRvSplitHalves$estimate, 3)
SSRvSplitHalves_p <- round(SSRvSplitHalves$p.value, 3)
meta_analysis_data %>% 
  filter(!is.na(ssr)) %>% 
  mutate(pt_label = ifelse(mean_split_corr < 0.5 | ssr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = ssr, y = mean_split_corr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(SSRvSplitHalves_r), ", p = ", toString(SSRvSplitHalves_p)), 
       x = "SSR", y = "Mean split-halves correlation")
ggsave("figs-pdf/FIG-ssr-vs-splithalves.pdf", units = "cm", width = 14, height = 14)
```


```{r ssr-vs-splithalves-by-adaptivity}
meta_analysis_data %>% 
  filter(!is.na(ssr)) %>% 
  ggplot(aes(x = ssr, y = mean_split_corr)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  ggpubr::stat_cor(p.accuracy = 0.001) +
  facet_grid(cols = vars(adaptivity)) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(x = "SSR", y = "Mean split-halves correlation")
```

### SSR vs split-halves for `N_CR` > 10 
NMM recommends 5 judgements per item, i.e. N_CR >= 9

```{r ssr-vs-splithalves-10}
SSRvSplitHalves_10 <- cor.test(meta_analysis_data_10$ssr, meta_analysis_data_10$mean_split_corr)
SSRvSplitHalves_r_10 <- round(SSRvSplitHalves_10$estimate, 3)
SSRvSplitHalves_p_10 <- round(SSRvSplitHalves_10$p.value, 3)
meta_analysis_data_10 %>% 
  filter(!is.na(ssr)) %>% 
  mutate(pt_label = ifelse(mean_split_corr < 0.5 | ssr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = ssr, y = mean_split_corr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(SSRvSplitHalves_r_10), ", p = ", toString(SSRvSplitHalves_p_10)), 
       x = "SSR", y = "Mean split-halves correlation")
ggsave("figs-pdf/FIG-ssr-vs-splithalves-10.pdf", units = "cm", width = 14, height = 14)
```

### SSR vs split-halves for `N_CR` >= 20 
Jones recommends 10 judgements per item, i.e. N_CR >= 19

```{r ssr-vs-splithalves-20}
SSRvSplitHalves_20 <- cor.test(meta_analysis_data_20$ssr, meta_analysis_data_20$mean_split_corr)
SSRvSplitHalves_r_20 <- round(SSRvSplitHalves_20$estimate, 3)
SSRvSplitHalves_p_20 <- round(SSRvSplitHalves_20$p.value, 3)
meta_analysis_data_20 %>% 
  filter(!is.na(ssr)) %>% 
  mutate(pt_label = ifelse(mean_split_corr < 0.5 | ssr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = ssr, y = mean_split_corr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(SSRvSplitHalves_r_20), ", p = ", toString(SSRvSplitHalves_p_20)), 
       x = "SSR", y = "Mean split-halves correlation")
ggsave("figs-pdf/FIG-ssr-vs-splithalves-20.pdf", units = "cm", width = 14, height = 14)
```

### SSR vs split-halves for `N_CR` >= 34 
Verhavert et al. 2019 recommend 17 judgements per item, i.e. N_CR >= 34

```{r ssr-vs-splithalves-34}
SSRvSplitHalves_34 <- cor.test(meta_analysis_data_34$ssr, meta_analysis_data_34$mean_split_corr)
SSRvSplitHalves_r_34 <- round(SSRvSplitHalves_34$estimate, 3)
SSRvSplitHalves_p_34 <- round(SSRvSplitHalves_34$p.value, 3)
meta_analysis_data_34 %>% 
  filter(!is.na(ssr)) %>% 
  mutate(pt_label = ifelse(mean_split_corr < 0.5 | ssr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = ssr, y = mean_split_corr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(SSRvSplitHalves_r_34), ", p = ", toString(SSRvSplitHalves_p_34)), 
       x = "SSR", y = "Mean split-halves correlation")
ggsave("figs-pdf/FIG-ssr-vs-splithalves-34.pdf", units = "cm", width = 14, height = 14)
```

It is very rare for the split-halves measure to be higher than SSR:

```{r}
meta_analysis_data %>% 
  filter(mean_split_corr > ssr) %>% select(judging_session, starts_with("observed"), mean_split_corr, ssr) %>% 
  basic_kable(digits = 3)
```

## Low mean split-halves reliability

These sessions have split-halves reliability below 0.7.

The `N_CR` column shows the "number of comparisons per representation", i.e. `N_C/N_R`.

```{r}
low_splithalves <- meta_analysis_data %>% 
  filter(mean_split_corr < 0.7) %>% 
  select(judging_session, starts_with("observed"), mean_split_corr, ssr) %>% 
  purrr::set_names(~ str_remove(., "observed_")) %>% 
  mutate(N_CR = N_C/N_R, .after = "N_C") %>% 
  arrange(mean_split_corr)

low_splithalves %>% 
  basic_kable(digits = 3)
```

Omitting the three sessions with N_CR = 400:

```{r low-splithaves}
low_splithalves %>% 
  ggplot(aes(y = mean_split_corr, x = N_CR)) +
  geom_point() +
  xlim(c(0,100))
```

# EloChoice

Here we look at how the EloChoice results compare with the Bradley-Terry ones.

## Reliability

First of all, how does the measure of reliability in EloChoice compare with SSR/split-halves?

There are two reliability measures proposed for EloChoice: see discussion in https://cran.r-project.org/web/packages/EloChoice/vignettes/EloChoice-tutorial.html

We use the weighted version, $R'$, computed based on 1000 iterations.

### SSR vs EloChoice

```{r ssr-vs-elo}
SSRvElo <- cor.test(meta_analysis_data$ssr, meta_analysis_data$mean_eloR_weighted, use="complete.obs")
SSRvElo_r <- round(SSRvElo$estimate, 3)
SSRvElo_p <- round(SSRvElo$p.value, 3)
meta_analysis_data %>% 
  filter(!is.na(mean_eloR_weighted)) %>% 
  mutate(pt_label = ifelse(mean_eloR_weighted < 0.5 | ssr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = ssr, y = mean_eloR_weighted, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_point() +
  geom_smooth(method = "lm", formula = "y ~ x") +
  ggrepel::geom_text_repel(alpha = 0.3) +
  geom_abline(slope = 1, intercept = 0) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(SSRvElo_r), ", p = ", toString(SSRvElo_p)), 
       x = "SSR", y = "Mean EloChoice R (weighted)")
```

### Split-halves vs EloChoice

```{r splithalves-vs-elo}
ElovSplitHalves <- cor.test(meta_analysis_data$mean_eloR_weighted, meta_analysis_data$mean_split_corr)
ElovSplitHalves_r <- round(ElovSplitHalves$estimate, 3)
ElovSplitHalves_p <- round(ElovSplitHalves$p.value, 3)
meta_analysis_data %>% 
  filter(!is.na(mean_eloR_weighted)) %>% 
  mutate(pt_label = ifelse(mean_eloR_weighted < 0.5 | mean_split_corr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = mean_split_corr, y = mean_eloR_weighted, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_point() +
  geom_smooth(method = "lm", formula = "y ~ x") +
  ggrepel::geom_text_repel(alpha = 0.3) +
  geom_abline(slope = 1, intercept = 0) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(ElovSplitHalves_r), ", p = ", toString(ElovSplitHalves_p)),
       x = "Mean split-halves", y = "Mean EloChoice R (weighted)")
```

### Split-halves vs EloChoice `N_CR` >= 10 

```{r splithalves-vs-elo-10}
ElovSplitHalves_10 <- cor.test(meta_analysis_data_10$mean_eloR_weighted, meta_analysis_data_10$mean_split_corr)
ElovSplitHalves_r_10 <- round(ElovSplitHalves_10$estimate, 3)
ElovSplitHalves_p_10 <- round(ElovSplitHalves_10$p.value, 3)
meta_analysis_data_10 %>% 
  filter(!is.na(mean_eloR_weighted)) %>% 
  mutate(pt_label = ifelse(mean_eloR_weighted < 0.5 | mean_split_corr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = mean_split_corr, y = mean_eloR_weighted, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_point() +
  geom_smooth(method = "lm", formula = "y ~ x") +
  ggrepel::geom_text_repel(alpha = 0.3) +
  geom_abline(slope = 1, intercept = 0) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(ElovSplitHalves_r_10), ", p = ", toString(ElovSplitHalves_p_10)),
       x = "Mean split-halves", y = "Mean EloChoice R (weighted)")
```

### Split-halves vs EloChoice `N_CR` >= 20 

```{r splithalves-vs-elo-20}
ElovSplitHalves_20 <- cor.test(meta_analysis_data_20$mean_eloR_weighted, meta_analysis_data_20$mean_split_corr)
ElovSplitHalves_r_20 <- round(ElovSplitHalves_20$estimate, 3)
ElovSplitHalves_p_20 <- round(ElovSplitHalves_20$p.value, 3)
meta_analysis_data_20 %>% 
  filter(!is.na(mean_eloR_weighted)) %>% 
  mutate(pt_label = ifelse(mean_eloR_weighted < 0.5 | mean_split_corr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = mean_split_corr, y = mean_eloR_weighted, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_point() +
  geom_smooth(method = "lm", formula = "y ~ x") +
  ggrepel::geom_text_repel(alpha = 0.3) +
  geom_abline(slope = 1, intercept = 0) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(ElovSplitHalves_r_20), ", p = ", toString(ElovSplitHalves_p_20)),
       x = "Mean split-halves", y = "Mean EloChoice R (weighted)")
```

### Split-halves vs EloChoice `N_CR` >= 34 

```{r splithalves-vs-elo-34}
ElovSplitHalves_34 <- cor.test(meta_analysis_data_34$mean_eloR_weighted, meta_analysis_data_34$mean_split_corr)
ElovSplitHalves_r_34 <- round(ElovSplitHalves_34$estimate, 3)
ElovSplitHalves_p_34 <- round(ElovSplitHalves_34$p.value, 3)
meta_analysis_data_34 %>% 
  filter(!is.na(mean_eloR_weighted)) %>% 
  mutate(pt_label = ifelse(mean_eloR_weighted < 0.5 | mean_split_corr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = mean_split_corr, y = mean_eloR_weighted, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_point() +
  geom_smooth(method = "lm", formula = "y ~ x") +
  ggrepel::geom_text_repel(alpha = 0.3) +
  geom_abline(slope = 1, intercept = 0) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(ElovSplitHalves_r_34), ", p = ", toString(ElovSplitHalves_p_34)),
       x = "Mean split-halves", y = "Mean EloChoice R (weighted)")
```

## Scores

How do the scores produced by EloChoice compare with those from Bradley-Terry?

Previous work has found high agreement:

* 0.95 and 0.94 in Clark et al. (2018) https://doi.org/10.1371/journal.pone.0190393
* Kendall's tau score of 0.96 for rank orders in Gray et al. (2022) https://doi.org/10.48550/arXiv.2204.01805

```{r elobtmcorr-histogram}
meta_analysis_data %>% 
  filter(!is.na(elo_btm_correlation)) %>% 
  ggplot(aes(x = elo_btm_correlation)) +
  geom_histogram(binwidth = 0.01)
```

```{r}
meta_analysis_data %>% 
  filter(elo_btm_correlation < 0.9) %>% 
  arrange(-elo_btm_correlation) %>% 
  select(elo_btm_correlation, judging_session, starts_with("observed_"), mean_eloR_weighted, mean_split_corr, ssr) %>% 
  basic_kable()
```

The $R'$ measure is quite low for all of those.

Looking at the relationship of $R'$ with the Elo-BTM correlation:

```{r elo-reliability-vs-btmcorr}
meta_analysis_data %>% 
  filter(!is.na(elo_btm_correlation)) %>% 
  ggplot(aes(x = mean_eloR_weighted, y = elo_btm_correlation)) +
  geom_point()
```

It's odd that there are some judging sessions with low $R'$ but high correlation of Elo and BTM scores.

```{r}
meta_analysis_data %>% 
  filter(elo_btm_correlation > 0.95, mean_eloR_weighted < 0.6) %>% 
  arrange(-elo_btm_correlation) %>% 
  select(elo_btm_correlation, judging_session, starts_with("observed_"), mean_eloR_weighted, mean_split_corr, ssr) %>% 
  basic_kable()
```

Those ones all seem to have a small number of representations - so perhaps the $R'$ measure is systematically underestimating reliability in those cases.


# Proportion of correct judgements

The proportion of judgements that are consistent with the final rank order relate more strongly to SSR and Elo than to split-halves reliability.

```{r}
prop_correct_model <- lm(prop_correct_judgements ~ ssr + mean_split_corr + mean_eloR, data = reliability_stats)
summary(prop_correct_model)
```

## Proportion of correction judgements vs SSR
```{r prop-vs-ssr}
PropvsSSR <- cor.test(meta_analysis_data$prop_correct_judgements, meta_analysis_data$ssr, use = "complete.obs")
PropvsSSR_r <- round(PropvsSSR$estimate, 3)
PropvsSSR_p <- round(PropvsSSR$p.value, 3)
meta_analysis_data %>% 
  filter(!is.na(prop_correct_judgements)) %>% 
  mutate(pt_label = ifelse(prop_correct_judgements < 0.5 | ssr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = prop_correct_judgements, y = ssr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(PropvsSSR_r), ", p = ", toString(PropvsSSR_p)), 
       x = "Proportion of correction judgements", y = "SSR")
ggsave("figs-pdf/FIG-prop-vs-ssr.pdf", units = "cm", width = 14, height = 14)
```

## Proportion of correction judgements vs Elo
```{r prop-vs-elo}
PropvsElo <- cor.test(meta_analysis_data$prop_correct_judgements, meta_analysis_data$mean_eloR_weighted, use = "complete.obs")
PropvsElo_r <- round(PropvsElo$estimate, 3)
PropvsElo_p <- round(PropvsElo$p.value, 3)
meta_analysis_data %>% 
  filter(!is.na(prop_correct_judgements)) %>% 
  mutate(pt_label = ifelse(prop_correct_judgements < 0.5 | ssr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = prop_correct_judgements, y = mean_eloR_weighted, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(PropvsElo_r), ", p = ", toString(PropvsElo_p)), 
       x = "Proportion of correction judgements", y = "Elo")
ggsave("figs-pdf/FIG-prop-vs-elo.pdf", units = "cm", width = 14, height = 14)
```

## Proportion of correction judgements vs split-halves

```{r prop-vs-splithalves}
PropvsSplithalves <- cor.test(meta_analysis_data$prop_correct_judgements, meta_analysis_data$mean_split_corr, use = "complete.obs")
PropvsSplithalves_r <- round(PropvsSplithalves$estimate, 3)
PropvsSplithalves_p <- round(PropvsSplithalves$p.value, 3)
meta_analysis_data %>% 
  filter(!is.na(prop_correct_judgements)) %>% 
  mutate(pt_label = ifelse(prop_correct_judgements < 0.5 | ssr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = prop_correct_judgements, y = mean_split_corr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(PropvsSplithalves_r), ", p = ", toString(PropvsSplithalves_p)), 
       x = "Proportion of correction judgements", y = "Split halves")
ggsave("figs-pdf/FIG-prop-vs-splithalves.pdf", units = "cm", width = 14, height = 14)
```

### Proportion of correction judgements vs split-halves for `N_CR` >= 10 

```{r prop-vs-splithalves-10}
PropvsSplithalves_10 <- cor.test(meta_analysis_data_10$prop_correct_judgements, meta_analysis_data_10$mean_split_corr, use = "complete.obs")
PropvsSplithalves_r_10 <- round(PropvsSplithalves_10$estimate, 3)
PropvsSplithalves_p_10 <- round(PropvsSplithalves_10$p.value, 3)
meta_analysis_data_10 %>% 
  filter(!is.na(prop_correct_judgements)) %>% 
  mutate(pt_label = ifelse(prop_correct_judgements < 0.5 | ssr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = prop_correct_judgements, y = mean_split_corr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(PropvsSplithalves_r_10), ", p = ", toString(PropvsSplithalves_p_10)), 
       x = "Proportion of correction judgements", y = "Split halves")
ggsave("figs-pdf/FIG-prop-vs-splithalves-10.pdf", units = "cm", width = 14, height = 14)
```

### Proportion of correction judgements vs split-halves for `N_CR` >= 20 

```{r prop-vs-splithalves-20}
PropvsSplithalves_20 <- cor.test(meta_analysis_data_20$prop_correct_judgements, meta_analysis_data_20$mean_split_corr, use = "complete.obs")
PropvsSplithalves_r_20 <- round(PropvsSplithalves_20$estimate, 3)
PropvsSplithalves_p_20 <- round(PropvsSplithalves_20$p.value, 3)
meta_analysis_data_20 %>% 
  filter(!is.na(prop_correct_judgements)) %>% 
  mutate(pt_label = ifelse(prop_correct_judgements < 0.5 | ssr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = prop_correct_judgements, y = mean_split_corr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(PropvsSplithalves_r_20), ", p = ", toString(PropvsSplithalves_p_20)), 
       x = "Proportion of correction judgements", y = "Split halves")
ggsave("figs-pdf/FIG-prop-vs-splithalves-20.pdf", units = "cm", width = 14, height = 14)
```

### Proportion of correction judgements vs split-halves for `N_CR` >= 34 

```{r prop-vs-splithalves-34}
PropvsSplithalves_34 <- cor.test(meta_analysis_data_34$prop_correct_judgements, meta_analysis_data_34$mean_split_corr, use = "complete.obs")
PropvsSplithalves_r_34 <- round(PropvsSplithalves_34$estimate, 3)
PropvsSplithalves_p_34 <- round(PropvsSplithalves_34$p.value, 3)
meta_analysis_data_34 %>% 
  filter(!is.na(prop_correct_judgements)) %>% 
  mutate(pt_label = ifelse(prop_correct_judgements < 0.5 | ssr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = prop_correct_judgements, y = mean_split_corr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(PropvsSplithalves_r_34), ", p = ", toString(PropvsSplithalves_p_34)), 
       x = "Proportion of correction judgements", y = "Split halves")
ggsave("figs-pdf/FIG-prop-vs-splithalves-34.pdf", units = "cm", width = 14, height = 14)
```
