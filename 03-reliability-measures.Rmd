---
title: 'CJ meta-analysis: reliability measures'
author: "George Kinnear"
date: "2022-08-22"
output: github_document
always_allow_html: true
---

```{r setup, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.path='figs-web/03-reliability-measures/')
knitr::opts_chunk$set(dpi=300,fig.width=7)

library("readr")
library("gghalves")

# for plotting
theme_set(theme_minimal())
#library(patchwork)

library(knitr)
library(kableExtra)
basic_kable = function(df, ...) {
  df %>% 
    kable(...) %>%
    kable_styling(bootstrap_options = "striped", full_width = F)
}
```

# About the sample

```{r}
cj_sessions <- read_csv("data/00-judging_sessions_summary.csv", show_col_types = FALSE)
reliability_stats <- read_csv("data/01-meta-analysis-data.csv", show_col_types = FALSE)

meta_analysis_data <- cj_sessions %>% 
  left_join(reliability_stats, by = "judging_session")
```

```{r}
meta_analysis_data %>% 
  mutate(source = replace_na(source, "literature search")) %>% 
  janitor::tabyl(source) %>% 
  janitor::adorn_pct_formatting(digits = 0) %>% 
  basic_kable()
```


```{r}
meta_analysis_data %>% 
  janitor::tabyl(adaptivity) %>% 
  janitor::adorn_pct_formatting(digits = 0) %>% 
  basic_kable()
```


# SSR computation

There seems to be a problem with the way `sirt::btm` computes the SSR:

> `mle.rel <- 1 - v2 / v0`
> <https://github.com/alexanderrobitzsch/sirt/blame/d0afec2822740805476055add1ba6b8bd2f04a37/R/btm.R#L265>

The SSR should be nonnegative, but in some cases this formula can give negative results. It tends to be in close agreement with the true SSR for higher values, but can diverge when there is limited judgement data.

Here we see how the values compare in our sample - the `ssr` column is the faulty value from `sirt::btm`, while `ssr_alt` comes from computing the correct value as G^2 / (1+G^2).

```{r}
meta_analysis_data <- meta_analysis_data %>% 
  mutate(ssr_alt = sepG^2 / (1+sepG^2))

ssr_comparison_data <- meta_analysis_data %>% 
  transmute(judging_session, ssr, sepG, ssr_alt, ssr_diff = abs(ssr_alt - ssr)) %>% 
  arrange(-ssr_diff)

ssr_comparison_data %>% basic_kable()
```



```{r include=FALSE}
CperR <- round(cj_sessions$observed_N_C/cj_sessions$observed_N_R,0)
meta_analysis_data$CperR <- CperR
meta_analysis_data_34 <- meta_analysis_data[meta_analysis_data$CperR > 33,]
meta_analysis_data_20 <- meta_analysis_data[meta_analysis_data$CperR > 19,]
meta_analysis_data_10 <- meta_analysis_data[meta_analysis_data$CperR > 9,]
```


```{r}
ssr_comparison_data %>% 
  ggplot(aes(x = ssr_alt, y = ssr)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0) +
  labs(x = "Correct SSR", y = "sirt::btm value for SSR")
```

So for the following analyses, we use the correct value of the SSR instead.

```{r}
meta_analysis_data <- meta_analysis_data %>% 
  # keep both values around just in case
  mutate(ssr_bad = ssr) %>% 
  mutate(ssr_good = ssr_alt) %>% 
  # but pick the correct one for going forward
  mutate(ssr = ssr_good) %>% 
  # tidy up the presentation of the adaptivity field
  mutate(adaptivity = str_replace(adaptivity, "TRUE", "Adaptive") %>% str_replace("FALSE", "Non-adaptive"))
```


# SSR vs split-halves

```{r ssr-vs-splithalves}
SSRvSplitHalves <- cor.test(meta_analysis_data$ssr, meta_analysis_data$median_split_corr)
SSRvSplitHalves_r <- round(SSRvSplitHalves$estimate, 3)
SSRvSplitHalves_p <- round(SSRvSplitHalves$p.value, 3)
meta_analysis_data %>% 
  filter(!is.na(ssr)) %>% 
  mutate(pt_label = ifelse(median_split_corr < 0.5 | ssr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = ssr, y = median_split_corr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(SSRvSplitHalves_r), ", p = ", toString(SSRvSplitHalves_p)), 
       x = "SSR", y = "Median split-halves correlation")
ggsave("figs-pdf/FIG-ssr-vs-splithalves.pdf", units = "cm", width = 14, height = 14)
```


```{r ssr-vs-splithalves-by-adaptivity}
meta_analysis_data %>% 
  filter(!is.na(ssr)) %>% 
  ggplot(aes(x = ssr, y = median_split_corr)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  ggpubr::stat_cor(p.accuracy = 0.001) +
  facet_grid(cols = vars(adaptivity)) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(x = "SSR", y = "Median split-halves correlation")
```

### Using SSR of the halves

When forming split halves, we fit the Bradley-Terry model using data from each half of the judges. Since this is a subset of the data, the SSR for the split halves will almost certainly be lower than the SSR for the full dataset.

One way to interpret reliability is "how likely would I be to get results like these if I repeated the process?". The split-halves approach answers this by comparing the scores from two subsets of the judges, A and B. If we imagine instead that group A was the only one that we collected data from, then the SSR from judge group A would be another way to estimate the "reliability", so should bear some relation to the correlation of group A's scores with group B's (imagining group B as "repeating the process" with a new group of judges).

So here, rather than using the SSR of the full dataset on the x-axis, we use only the SSR of one of the split halves. Plotted here is every one of the 100 splits from each of the studies in the sample:

```{r}
if(file.exists("data-cache/split_halves_summary.csv")) {
  splits_stats <- read_csv("data-cache/split_halves_summary.csv", show_col_types = FALSE)
} else {
  files <- list.files(pattern = "split_halves.csv", recursive = TRUE)
  
  splits <- vroom::vroom("data-cache/Bisson2019/split_halves.csv") %>% 
    mutate(data = purrr::map(path, ~ vroom::vroom(.x) %>% select(contains("theta"))))
  
  splits <- tibble(file = list.files(pattern = "split_halves.csv", recursive = TRUE)) %>% 
    #head(n = 10) %>% 
    mutate(file_contents = purrr::map(file, ~ vroom::vroom(.x, show_col_types = FALSE))) %>% 
    unnest(file_contents) %>% 
    mutate(data = purrr::map(path, ~ vroom::vroom(.x, show_col_types = FALSE) %>% select(contains("theta"))))
  
  splits_stats <- splits %>% 
    mutate(
      halves_info = purrr::map(data,
                               function(df) {
                                 df %>%
                                   summarise(
                                     G_x = sd(theta.x) / sqrt(mean(se.theta.x ^ 2)),
                                     ssr_x = G_x ^ 2 / (1 + G_x ^ 2),
                                     G_y = sd(theta.y) / sqrt(mean(se.theta.y ^ 2)),
                                     ssr_y = G_y ^ 2 / (1 + G_y ^ 2)
                                   )
                               })
    ) %>% 
    unnest(cols = c(halves_info))
  
  splits_stats %>% write_csv("data-cache/split_halves_summary.csv")
}



splits_stats %>% 
  ggplot(aes(x = ssr_x, y = split_corr)) +
  geom_point() +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  ggpubr::stat_cor(p.accuracy = 0.001)
```

Here we omit studies where the number of comparisons is too low for the split half to meet the recommendations set out by Verhavert et al., and add the prediction interval to the plot:

```{r}
splits_stats %>% 
  separate(file, into = c(NA, "study", NA), sep = "/") %>% 
  left_join(meta_analysis_data, by = c("study" = "judging_session")) %>% 
  # restrict to those studies that gathered enough data for each split half to meet the Verhavert recommended N_CR
  # note: results are quite different if you do/don't use this filter!
  filter(observed_N_C/observed_N_R > 2*17) %>% 
  # add the prediction interval
  cbind(
    predict(lm(split_corr ~ ssr_x, data = .), interval = "prediction")
  ) %>% 
  ggplot(aes(x = ssr_x, y = split_corr)) +
  geom_point(aes(colour = study), alpha = 0.3) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), fill = "blue", alpha = 0.2) +
  ggpubr::stat_cor(p.accuracy = 0.001) +
  theme(legend.position = "none")
```

This is then a bit like the plot by Verhavert et al. (2018), appearing in the bottom middle (or bottom right) panel of their Figure C1, and similarly shows a fairly "y=x" trend line:

```{r}
splits_stats %>% 
  separate(file, into = c(NA, "study", NA), sep = "/") %>% 
  left_join(meta_analysis_data, by = c("study" = "judging_session")) %>% 
  # restrict to those studies that gathered enough data for each split half to meet the Verhavert recommended N_CR
  # note: results are quite different if you do/don't use this filter!
  filter(observed_N_C/observed_N_R > 2*17) %>% 
  # these points are the outliers:
  # filter(!str_detect(study, "Spehar")) %>% 
  group_by(study) %>% 
  #summarise(ssr_x = mean(ssr_x), split_corr = median(split_corr)) %>% 
  #should this not be the median?
  summarise(ssr_x = median(ssr_x), split_corr = median(split_corr)) %>% 
  # add the prediction interval
  cbind(
    predict(lm(split_corr ~ ssr_x, data = .), interval = "prediction")
  ) %>% 
  ggplot(aes(x = ssr_x, y = split_corr)) +
  geom_point(alpha = 0.9) +
  #ggrepel::geom_text_repel(aes(label = study), alpha = 0.9) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), fill = "blue", alpha = 0.2) +
  ggpubr::stat_cor(p.accuracy = 0.001) +
  geom_abline(slope = 1) +
  expand_limits(x = 0, y = 0) +
  theme(legend.position = "bottom")
```
### SSR vs split-halves for half the assessors

```{r ssr_x-vs-splithalves-all}
  
  splits_stats_all <- splits_stats %>% 
  separate(file, into = c(NA, "study", NA), sep = "/") %>%
  left_join(meta_analysis_data, by = c("study" = "judging_session")) %>% 
  filter(!str_detect(study, "Spehar")) %>% 
  group_by(study) %>% 
  summarise(ssr_x = median(ssr_x), median_split_corr = median(split_corr)) %>% 
  filter(!is.na(ssr_x)) %>%  as.data.frame
  
  SSR_xvSplitHalves_all <- cor.test(splits_stats_all$ssr_x,
                                  splits_stats_all$median_split_corr)
  SSR_xvSplitHalves_r_all <- round(SSR_xvSplitHalves_all$estimate, 3)
  SSR_xvSplitHalves_p_all <- round(SSR_xvSplitHalves_all$p.value, 3)

  splits_stats_all %>% 
  mutate(pt_label = ifelse(median_split_corr < 0.5 | ssr_x < 0.7, study, "")) %>% 
  ggplot(aes(x = ssr_x, y = median_split_corr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(#title = paste("r = ", toString(SSR_xvSplitHalves_r_all), ", p = ", toString(SSR_xvSplitHalves_p_all)), 
       x = "SSR", y = "Median split-halves correlation")
ggsave("figs-pdf/FIG-ssr_x-vs-splithalves-all.pdf", units = "cm", width = 14, height = 14)
```



### SSR vs split-halves for `N_CR` => 10 
NMM recommends 5 judgements per item, i.e. N_CR > 9. With SSR based on all assessors' comparisons:

```{r ssr-vs-splithalves-10}
SSRvSplitHalves_10 <- cor.test(meta_analysis_data_10$ssr, meta_analysis_data_10$median_split_corr)
SSRvSplitHalves_r_10 <- round(SSRvSplitHalves_10$estimate, 3)
SSRvSplitHalves_p_10 <- round(SSRvSplitHalves_10$p.value, 3)
meta_analysis_data_10 %>% 
  filter(!is.na(ssr)) %>% 
  mutate(pt_label = ifelse(median_split_corr < 0.5 | ssr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = ssr, y = median_split_corr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(SSRvSplitHalves_r_10), ", p = ", toString(SSRvSplitHalves_p_10)), 
       x = "SSR", y = "Median split-halves correlation")
ggsave("figs-pdf/FIG-ssr-vs-splithalves-10.pdf", units = "cm", width = 14, height = 14)
```

And here N_CR > 9 with SSR based on half of assessors' comparisons:

```{r ssr_x-vs-splithalves-10}
  
  splits_stats_10 <- splits_stats %>% 
  separate(file, into = c(NA, "study", NA), sep = "/") %>%
  left_join(meta_analysis_data, by = c("study" = "judging_session")) %>% 
  filter(!str_detect(study, "Spehar")) %>% 
  filter(CperR > 9) %>% 
  group_by(study) %>% 
  summarise(ssr_x = median(ssr_x), median_split_corr = median(split_corr)) %>% 
  filter(!is.na(ssr_x)) %>%  as.data.frame
  
  SSR_xvSplitHalves_10 <- cor.test(splits_stats_10$ssr_x,
                                  splits_stats_10$median_split_corr)
  SSR_xvSplitHalves_r_10 <- round(SSR_xvSplitHalves_10$estimate, 3)
  SSR_xvSplitHalves_p_10 <- round(SSR_xvSplitHalves_10$p.value, 3)

  splits_stats_10 %>% 
  mutate(pt_label = ifelse(median_split_corr < 0.5 | ssr_x < 0.7, study, "")) %>% 
  ggplot(aes(x = ssr_x, y = median_split_corr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(#title = paste("r = ", toString(SSR_xvSplitHalves_r_10), ", p = ", toString(SSR_xvSplitHalves_p_10)), 
       x = "SSR", y = "Median split-halves correlation")
ggsave("figs-pdf/FIG-ssr_x-vs-splithalves-10.pdf", units = "cm", width = 14, height = 14)
```


### SSR vs split-halves for `N_CR` >= 20 
Jones recommends 10 judgements per item, i.e. N_CR > 19 with SSR based on all assessors' comparisons:

```{r ssr-vs-splithalves-20}
SSRvSplitHalves_20 <- cor.test(meta_analysis_data_20$ssr, meta_analysis_data_20$median_split_corr)
SSRvSplitHalves_r_20 <- round(SSRvSplitHalves_20$estimate, 3)
SSRvSplitHalves_p_20 <- round(SSRvSplitHalves_20$p.value, 3)
meta_analysis_data_20 %>% 
  filter(!is.na(ssr)) %>% 
  mutate(pt_label = ifelse(median_split_corr < 0.5 | ssr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = ssr, y = median_split_corr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(SSRvSplitHalves_r_20), ", p = ", toString(SSRvSplitHalves_p_20)), 
       x = "SSR", y = "Median split-halves correlation")
ggsave("figs-pdf/FIG-ssr-vs-splithalves-20.pdf", units = "cm", width = 14, height = 14)
```
And here N_CR > 19 with SSR based on half of assessors' comparisons:

```{r ssr_x-vs-splithalves-20}
  
  splits_stats_20 = splits_stats %>% 
  separate(file, into = c(NA, "study", NA), sep = "/") %>% 
  left_join(meta_analysis_data, by = c("study" = "judging_session")) %>% 
  filter(!str_detect(study, "Spehar")) %>% 
  filter(CperR > 19) %>% 
  group_by(study) %>% 
  summarise(ssr_x = median(ssr_x), median_split_corr = median(split_corr)) %>% 
  filter(!is.na(ssr_x)) %>%  as.data.frame
  
  SSR_xvSplitHalves_20 <- cor.test(splits_stats_20$ssr_x,
                                  splits_stats_20$median_split_corr)
  SSR_xvSplitHalves_r_20 <- round(SSR_xvSplitHalves_20$estimate, 3)
  SSR_xvSplitHalves_p_20 <- round(SSR_xvSplitHalves_20$p.value, 3)

  splits_stats_20 %>% 
  mutate(pt_label = ifelse(median_split_corr < 0.5 | ssr_x < 0.7, study, "")) %>% 
  ggplot(aes(x = ssr_x, y = median_split_corr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(#title = paste("r = ", toString(SSR_xvSplitHalves_r_20), ", p = ", toString(SSR_xvSplitHalves_p_20)), 
       x = "SSR", y = "Median split-halves correlation")
ggsave("figs-pdf/FIG-ssr_x-vs-splithalves-20.pdf", units = "cm", width = 14, height = 14)
```



### SSR vs split-halves for `N_CR` >= 34 
Verhavert et al. 2019 recommend 17 judgements per item, i.e. N_CR > 33  with SSR based on all assessors' comparisons:

```{r ssr-vs-splithalves-34}
SSRvSplitHalves_34 <- cor.test(meta_analysis_data_34$ssr, meta_analysis_data_34$median_split_corr)
SSRvSplitHalves_r_34 <- round(SSRvSplitHalves_34$estimate, 3)
SSRvSplitHalves_p_34 <- round(SSRvSplitHalves_34$p.value, 3)
meta_analysis_data_34 %>% 
  filter(!is.na(ssr)) %>% 
  mutate(pt_label = ifelse(median_split_corr < 0.5 | ssr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = ssr, y = median_split_corr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(SSRvSplitHalves_r_34), ", p = ", toString(SSRvSplitHalves_p_34)), 
       x = "SSR", y = "Median split-halves correlation")
ggsave("figs-pdf/FIG-ssr-vs-splithalves-34.pdf", units = "cm", width = 14, height = 14)
```
And here N_CR > 33 with SSR based on half of assessors' comparisons:

```{r ssr_x-vs-splithalves-34}
  
  splits_stats_34 = splits_stats %>% 
  separate(file, into = c(NA, "study", NA), sep = "/") %>% 
  left_join(meta_analysis_data, by = c("study" = "judging_session")) %>% 
  filter(!str_detect(study, "Spehar")) %>% 
  filter(CperR > 33) %>% 
  group_by(study) %>% 
  summarise(ssr_x = median(ssr_x), median_split_corr = median(split_corr)) %>% 
  filter(!is.na(ssr_x)) %>%  as.data.frame
  
  SSR_xvSplitHalves_34 <- cor.test(splits_stats_34$ssr_x,
                                  splits_stats_34$median_split_corr)
  SSR_xvSplitHalves_r_34 <- round(SSR_xvSplitHalves_34$estimate, 3)
  SSR_xvSplitHalves_p_34 <- round(SSR_xvSplitHalves_34$p.value, 3)

  splits_stats_34 %>% 
  mutate(pt_label = ifelse(median_split_corr < 0.5 | ssr_x < 0.7, study, "")) %>% 
  ggplot(aes(x = ssr_x, y = median_split_corr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(#title = paste("r = ", toString(SSR_xvSplitHalves_r_34), ", p = ", toString(SSR_xvSplitHalves_p_34)), 
       x = "SSR", y = "Median split-halves correlation")
ggsave("figs-pdf/FIG-ssr_x-vs-splithalves-34.pdf", units = "cm", width = 14, height = 14)
```


It is very rare for the split-halves measure to be higher than SSR:

```{r}
meta_analysis_data %>% 
  filter(median_split_corr > ssr) %>% select(judging_session, starts_with("observed"), median_split_corr, ssr) %>% 
  basic_kable(digits = 3)
```

## Low median split-halves reliability

These sessions have split-halves reliability below 0.7.

The `N_CR` column shows the "number of comparisons per representation", i.e. `N_C/N_R`.

```{r}
low_splithalves <- meta_analysis_data %>% 
  filter(median_split_corr < 0.7) %>% 
  select(judging_session, starts_with("observed"), median_split_corr, ssr) %>% 
  purrr::set_names(~ str_remove(., "observed_")) %>% 
  mutate(N_CR = N_C/N_R, .after = "N_C") %>% 
  arrange(median_split_corr)

low_splithalves %>% 
  basic_kable(digits = 3)
```

Omitting the three sessions with N_CR = 400:

```{r low-splithaves}
low_splithalves %>% 
  ggplot(aes(y = median_split_corr, x = N_CR)) +
  geom_point() +
  xlim(c(0,100))
```

# EloChoice

Here we look at how the EloChoice results compare with the Bradley-Terry ones.

## Reliability

First of all, how does the measure of reliability in EloChoice compare with SSR/split-halves?

There are two reliability measures proposed for EloChoice: see discussion in https://cran.r-project.org/web/packages/EloChoice/vignettes/EloChoice-tutorial.html

We use the weighted version, $R'$, computed based on 1000 iterations.

### SSR vs EloChoice

```{r ssr-vs-elo}
SSRvElo <- cor.test(meta_analysis_data$ssr, meta_analysis_data$mean_eloR_weighted, use="complete.obs")
SSRvElo_r <- round(SSRvElo$estimate, 3)
SSRvElo_p <- round(SSRvElo$p.value, 3)
meta_analysis_data %>% 
  filter(!is.na(mean_eloR_weighted)) %>% 
  mutate(pt_label = ifelse(mean_eloR_weighted < 0.5 | ssr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = ssr, y = mean_eloR_weighted, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_point() +
  geom_smooth(method = "lm", formula = "y ~ x") +
  ggrepel::geom_text_repel(alpha = 0.3) +
  geom_abline(slope = 1, intercept = 0) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(SSRvElo_r), ", p = ", toString(SSRvElo_p)), 
       x = "SSR", y = "Mean EloChoice R (weighted)")
```

### Split-halves vs EloChoice

```{r splithalves-vs-elo}
ElovSplitHalves <- cor.test(meta_analysis_data$mean_eloR_weighted, meta_analysis_data$median_split_corr)
ElovSplitHalves_r <- round(ElovSplitHalves$estimate, 3)
ElovSplitHalves_p <- round(ElovSplitHalves$p.value, 3)
meta_analysis_data %>% 
  filter(!is.na(mean_eloR_weighted)) %>% 
  mutate(pt_label = ifelse(mean_eloR_weighted < 0.5 | median_split_corr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = median_split_corr, y = mean_eloR_weighted, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_point() +
  geom_smooth(method = "lm", formula = "y ~ x") +
  ggrepel::geom_text_repel(alpha = 0.3) +
  geom_abline(slope = 1, intercept = 0) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(ElovSplitHalves_r), ", p = ", toString(ElovSplitHalves_p)),
       x = "Median split-halves", y = "Mean EloChoice R (weighted)")
```

### Split-halves vs EloChoice `N_CR` >= 10 

```{r splithalves-vs-elo-10}
ElovSplitHalves_10 <- cor.test(meta_analysis_data_10$mean_eloR_weighted, meta_analysis_data_10$median_split_corr)
ElovSplitHalves_r_10 <- round(ElovSplitHalves_10$estimate, 3)
ElovSplitHalves_p_10 <- round(ElovSplitHalves_10$p.value, 3)
meta_analysis_data_10 %>% 
  filter(!is.na(mean_eloR_weighted)) %>% 
  mutate(pt_label = ifelse(mean_eloR_weighted < 0.5 | median_split_corr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = median_split_corr, y = mean_eloR_weighted, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_point() +
  geom_smooth(method = "lm", formula = "y ~ x") +
  ggrepel::geom_text_repel(alpha = 0.3) +
  geom_abline(slope = 1, intercept = 0) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(ElovSplitHalves_r_10), ", p = ", toString(ElovSplitHalves_p_10)),
       x = "Median split-halves", y = "Mean EloChoice R (weighted)")
```

### Split-halves vs EloChoice `N_CR` >= 20 

```{r splithalves-vs-elo-20}
ElovSplitHalves_20 <- cor.test(meta_analysis_data_20$mean_eloR_weighted, meta_analysis_data_20$median_split_corr)
ElovSplitHalves_r_20 <- round(ElovSplitHalves_20$estimate, 3)
ElovSplitHalves_p_20 <- round(ElovSplitHalves_20$p.value, 3)
meta_analysis_data_20 %>% 
  filter(!is.na(mean_eloR_weighted)) %>% 
  mutate(pt_label = ifelse(mean_eloR_weighted < 0.5 | median_split_corr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = median_split_corr, y = mean_eloR_weighted, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_point() +
  geom_smooth(method = "lm", formula = "y ~ x") +
  ggrepel::geom_text_repel(alpha = 0.3) +
  geom_abline(slope = 1, intercept = 0) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(ElovSplitHalves_r_20), ", p = ", toString(ElovSplitHalves_p_20)),
       x = "Median split-halves", y = "Mean EloChoice R (weighted)")
```

### Split-halves vs EloChoice `N_CR` >= 34 

```{r splithalves-vs-elo-34}
ElovSplitHalves_34 <- cor.test(meta_analysis_data_34$mean_eloR_weighted, meta_analysis_data_34$median_split_corr)
ElovSplitHalves_r_34 <- round(ElovSplitHalves_34$estimate, 3)
ElovSplitHalves_p_34 <- round(ElovSplitHalves_34$p.value, 3)
meta_analysis_data_34 %>% 
  filter(!is.na(mean_eloR_weighted)) %>% 
  mutate(pt_label = ifelse(mean_eloR_weighted < 0.5 | median_split_corr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = median_split_corr, y = mean_eloR_weighted, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_point() +
  geom_smooth(method = "lm", formula = "y ~ x") +
  ggrepel::geom_text_repel(alpha = 0.3) +
  geom_abline(slope = 1, intercept = 0) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(ElovSplitHalves_r_34), ", p = ", toString(ElovSplitHalves_p_34)),
       x = "Median split-halves", y = "Mean EloChoice R (weighted)")
```

## Scores

How do the scores produced by EloChoice compare with those from Bradley-Terry?

Previous work has found high agreement:

* 0.95 and 0.94 in Clark et al. (2018) https://doi.org/10.1371/journal.pone.0190393
* Kendall's tau score of 0.96 for rank orders in Gray et al. (2022) https://doi.org/10.48550/arXiv.2204.01805

```{r elobtmcorr-histogram}
meta_analysis_data %>% 
  filter(!is.na(elo_btm_correlation)) %>% 
  ggplot(aes(x = elo_btm_correlation)) +
  geom_histogram(binwidth = 0.01)
```

```{r}
meta_analysis_data %>% 
  filter(elo_btm_correlation < 0.9) %>% 
  arrange(-elo_btm_correlation) %>% 
  select(elo_btm_correlation, judging_session, starts_with("observed_"), mean_eloR_weighted, median_split_corr, ssr) %>% 
  basic_kable()
```

The $R'$ measure is quite low for all of those.

Looking at the relationship of $R'$ with the Elo-BTM correlation:

```{r elo-reliability-vs-btmcorr}
meta_analysis_data %>% 
  filter(!is.na(elo_btm_correlation)) %>% 
  ggplot(aes(x = mean_eloR_weighted, y = elo_btm_correlation)) +
  geom_point()
```

It's odd that there are some judging sessions with low $R'$ but high correlation of Elo and BTM scores.

```{r}
meta_analysis_data %>% 
  filter(elo_btm_correlation > 0.95, mean_eloR_weighted < 0.6) %>% 
  arrange(-elo_btm_correlation) %>% 
  select(elo_btm_correlation, judging_session, starts_with("observed_"), mean_eloR_weighted, median_split_corr, ssr) %>% 
  basic_kable()
```

Those ones all seem to have a small number of representations - so perhaps the $R'$ measure is systematically underestimating reliability in those cases.


# Proportion of correct judgements

The proportion of judgements that are consistent with the final rank order relate more strongly to SSR and Elo than to split-halves reliability.

```{r}
prop_correct_model <- lm(prop_correct_judgements ~ ssr + median_split_corr + mean_eloR, data = reliability_stats)
summary(prop_correct_model)
```

## Proportion of correction judgements vs SSR
```{r prop-vs-ssr}
PropvsSSR <- cor.test(meta_analysis_data$prop_correct_judgements, meta_analysis_data$ssr, use = "complete.obs")
PropvsSSR_r <- round(PropvsSSR$estimate, 3)
PropvsSSR_p <- round(PropvsSSR$p.value, 3)
meta_analysis_data %>% 
  filter(!is.na(prop_correct_judgements)) %>% 
  mutate(pt_label = ifelse(prop_correct_judgements < 0.5 | ssr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = prop_correct_judgements, y = ssr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(#title = paste("r = ", toString(PropvsSSR_r), ", p = ", toString(PropvsSSR_p)), 
       x = "Proportion of correction judgements", y = "SSR")
ggsave("figs-pdf/FIG-prop-vs-ssr.pdf", units = "cm", width = 14, height = 14)
```

### SSR vs proportion correct for half the assessors

```{r ssr_x-vs-prop-all}
  
  splits_stats_three <- splits_stats_all %>%
  left_join(meta_analysis_data[,c(1,29,31)], by = c("study" = "judging_session"))     %>%  as.data.frame


  SSR_xvProp_all <- cor.test(splits_stats_three$ssr_x,
                                  splits_stats_three$prop_correct_judgements)
  SSR_xvProp_r_all <- round(SSR_xvProp_all$estimate, 3)
  SSR_xvProp_p_all <- round(SSR_xvProp_all$p.value, 3)

  splits_stats_three %>% 
  mutate(pt_label = ifelse(median_split_corr < 0.5 | ssr_x < 0.7, study, "")) %>% 
  ggplot(aes(y = ssr_x, x = median_split_corr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(#title = paste("r = ", toString(SSR_xvProp_r_all), ", p = ", toString(SSR_xvProp_p_all)), 
       y = "SSR", x = "Proportion of correct comparisons")
ggsave("figs-pdf/FIG-ssr_x-vs-prop-all.pdf", units = "cm", width = 14, height = 14)
```



### SSR vs proportion correct for `N_CR` >= 10 based on half of assessors' comparisons:

```{r ssr_x-vs-prop-10}
  
 splits_stats_three_10 <- splits_stats_three %>%
      filter(!str_detect(study, "Spehar")) %>% 
      filter(CperR > 9) %>% 
      filter(!is.na(ssr_x)) %>%  as.data.frame()

  SSR_xvProp_10 <- cor.test(splits_stats_three_10$ssr_x,
                                  splits_stats_three_10$median_split_corr)
  SSR_xvProp_r_10 <- round(SSR_xvProp_10$estimate, 3)
  SSR_xvProp_p_10 <- round(SSR_xvProp_10$p.value, 3)

  splits_stats_three_10 %>% 
  mutate(pt_label = ifelse(median_split_corr < 0.5 | ssr_x < 0.7, study, "")) %>% 
  ggplot(aes(y = ssr_x, x = median_split_corr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(#title = paste("r = ", toString(SSR_xvProp_r_10), ", p = ", toString(SSR_xvProp_p_10)), 
       y = "SSR", x = "Proportion of correct comparisons")
ggsave("figs-pdf/FIG-ssr_x-vs-prop-10.pdf", units = "cm", width = 14, height = 14)
```






### SSR vs proportion correct for `N_CR` >= 20 based on half of assessors' comparisons:

```{r ssr_x-vs-prop-20}
  
 splits_stats_three_20 <- splits_stats_three %>%
      filter(!str_detect(study, "Spehar")) %>% 
      filter(CperR > 19) %>% 
      filter(!is.na(ssr_x)) %>%  as.data.frame()

  SSR_xvProp_20 <- cor.test(splits_stats_three_20$ssr_x,
                                  splits_stats_20$median_split_corr)
  SSR_xvProp_r_20 <- round(SSR_xvProp_20$estimate, 3)
  SSR_xvProp_p_20 <- round(SSR_xvProp_20$p.value, 3)

  splits_stats_three_20 %>% 
  mutate(pt_label = ifelse(median_split_corr < 0.5 | ssr_x < 0.7, study, "")) %>% 
  ggplot(aes(y = ssr_x, x = median_split_corr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(#title = paste("r = ", toString(SSR_xvProp_r_20), ", p = ", toString(SSR_xvProp_p_20)), 
       y = "SSR", x = "Proportion of correct comparisons")
ggsave("figs-pdf/FIG-ssr_x-vs-prop-20.pdf", units = "cm", width = 14, height = 14)
```



### SSR vs proportion correct for `N_CR` >= 34 based on half of assessors' comparisons:

```{r ssr_x-vs-prop-34}
  
 splits_stats_three_34 <- splits_stats_three %>%
      filter(!str_detect(study, "Spehar")) %>% 
      filter(CperR > 33) %>% 
      filter(!is.na(ssr_x)) %>%  as.data.frame()

  SSR_xvProp_34 <- cor.test(splits_stats_three_34$ssr_x,
                                  splits_stats_34$median_split_corr)
  SSR_xvProp_r_34 <- round(SSR_xvProp_34$estimate, 3)
  SSR_xvProp_p_34 <- round(SSR_xvProp_34$p.value, 3)

  splits_stats_three_34 %>% 
  mutate(pt_label = ifelse(median_split_corr < 0.5 | ssr_x < 0.7, study, "")) %>% 
  ggplot(aes(y = ssr_x, x = median_split_corr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(SSR_xvProp_r_34), ", p = ", toString(SSR_xvProp_p_34)), 
       y = "SSR", x = "Proportion of correct comparisons")
ggsave("figs-pdf/FIG-ssr_x-vs-prop-34.pdf", units = "cm", width = 14, height = 14)
```


## Proportion of correction judgements vs Elo
```{r prop-vs-elo}
PropvsElo <- cor.test(meta_analysis_data$prop_correct_judgements, meta_analysis_data$mean_eloR_weighted, use = "complete.obs")
PropvsElo_r <- round(PropvsElo$estimate, 3)
PropvsElo_p <- round(PropvsElo$p.value, 3)
meta_analysis_data %>% 
  filter(!is.na(prop_correct_judgements)) %>% 
  mutate(pt_label = ifelse(prop_correct_judgements < 0.5 | ssr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = prop_correct_judgements, y = mean_eloR_weighted, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(PropvsElo_r), ", p = ", toString(PropvsElo_p)), 
       x = "Proportion of correction judgements", y = "Elo")
ggsave("figs-pdf/FIG-prop-vs-elo.pdf", units = "cm", width = 14, height = 14)
```

## Proportion of correction judgements vs split-halves

```{r prop-vs-splithalves}
PropvsSplithalves <- cor.test(meta_analysis_data$prop_correct_judgements, meta_analysis_data$median_split_corr, use = "complete.obs")
PropvsSplithalves_r <- round(PropvsSplithalves$estimate, 3)
PropvsSplithalves_p <- round(PropvsSplithalves$p.value, 3)
meta_analysis_data %>% 
  filter(!is.na(prop_correct_judgements)) %>% 
  mutate(pt_label = ifelse(prop_correct_judgements < 0.5 | ssr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = prop_correct_judgements, y = median_split_corr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(PropvsSplithalves_r), ", p = ", toString(PropvsSplithalves_p)), 
       x = "Proportion of correction judgements", y = "Split halves")
ggsave("figs-pdf/FIG-prop-vs-splithalves.pdf", units = "cm", width = 14, height = 14)
```

### Proportion of correction judgements vs split-halves for `N_CR` >= 10 

```{r prop-vs-splithalves-10}
PropvsSplithalves_10 <- cor.test(meta_analysis_data_10$prop_correct_judgements, meta_analysis_data_10$median_split_corr, use = "complete.obs")
PropvsSplithalves_r_10 <- round(PropvsSplithalves_10$estimate, 3)
PropvsSplithalves_p_10 <- round(PropvsSplithalves_10$p.value, 3)
meta_analysis_data_10 %>% 
  filter(!is.na(prop_correct_judgements)) %>% 
  mutate(pt_label = ifelse(prop_correct_judgements < 0.5 | ssr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = prop_correct_judgements, y = median_split_corr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(PropvsSplithalves_r_10), ", p = ", toString(PropvsSplithalves_p_10)), 
       x = "Proportion of correction judgements", y = "Split halves")
ggsave("figs-pdf/FIG-prop-vs-splithalves-10.pdf", units = "cm", width = 14, height = 14)
```

### Proportion of correction judgements vs split-halves for `N_CR` >= 20 

```{r prop-vs-splithalves-20}
PropvsSplithalves_20 <- cor.test(meta_analysis_data_20$prop_correct_judgements, meta_analysis_data_20$median_split_corr, use = "complete.obs")
PropvsSplithalves_r_20 <- round(PropvsSplithalves_20$estimate, 3)
PropvsSplithalves_p_20 <- round(PropvsSplithalves_20$p.value, 3)
meta_analysis_data_20 %>% 
  filter(!is.na(prop_correct_judgements)) %>% 
  mutate(pt_label = ifelse(prop_correct_judgements < 0.5 | ssr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = prop_correct_judgements, y = median_split_corr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(PropvsSplithalves_r_20), ", p = ", toString(PropvsSplithalves_p_20)), 
       x = "Proportion of correction judgements", y = "Split halves")
ggsave("figs-pdf/FIG-prop-vs-splithalves-20.pdf", units = "cm", width = 14, height = 14)
```

### Proportion of correction judgements vs split-halves for `N_CR` >= 34 

```{r prop-vs-splithalves-34}
PropvsSplithalves_34 <- cor.test(meta_analysis_data_34$prop_correct_judgements, meta_analysis_data_34$median_split_corr, use = "complete.obs")
PropvsSplithalves_r_34 <- round(PropvsSplithalves_34$estimate, 3)
PropvsSplithalves_p_34 <- round(PropvsSplithalves_34$p.value, 3)
meta_analysis_data_34 %>% 
  filter(!is.na(prop_correct_judgements)) %>% 
  mutate(pt_label = ifelse(prop_correct_judgements < 0.5 | ssr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = prop_correct_judgements, y = median_split_corr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(PropvsSplithalves_r_34), ", p = ", toString(PropvsSplithalves_p_34)), 
       x = "Proportion of correction judgements", y = "Split halves")
ggsave("figs-pdf/FIG-prop-vs-splithalves-34.pdf", units = "cm", width = 14, height = 14)
```
