---
title: 'CJ meta-analysis: reliability measures'
author: "George Kinnear"
date: "2022-08-22"
output: github_document
always_allow_html: true
---

```{r setup, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.path='figs-web/03-reliability-measures/')
knitr::opts_chunk$set(dpi=300,fig.width=7)

library("readr")
library("gghalves")

# for plotting
theme_set(theme_minimal())
#library(patchwork)

library(knitr)
library(kableExtra)
basic_kable = function(df, ...) {
  df %>% 
    kable(...) %>%
    kable_styling(bootstrap_options = "striped", full_width = F)
}
```

# About the sample

```{r}
cj_sessions <- read_csv("data/00-judging_sessions_summary.csv", show_col_types = FALSE)
reliability_stats <- read_csv("data/01-meta-analysis-data.csv", show_col_types = FALSE)

meta_analysis_data <- cj_sessions %>% 
  left_join(reliability_stats, by = "judging_session")
```

```{r}
meta_analysis_data %>% 
  mutate(source = replace_na(source, "literature search")) %>% 
  janitor::tabyl(source) %>% 
  janitor::adorn_pct_formatting(digits = 0) %>% 
  basic_kable()
```


```{r}
meta_analysis_data %>% 
  janitor::tabyl(adaptivity) %>% 
  janitor::adorn_pct_formatting(digits = 0) %>% 
  basic_kable()
```


# SSR computation

There seems to be a problem with the way `sirt::btm` computes the SSR:

> `mle.rel <- 1 - v2 / v0`
> <https://github.com/alexanderrobitzsch/sirt/blame/d0afec2822740805476055add1ba6b8bd2f04a37/R/btm.R#L265>

The SSR should be nonnegative, but in some cases this formula can give negative results. It tends to be in close agreement with the true SSR for higher values, but can diverge when there is limited judgement data.

Here we see how the values compare in our sample - the `ssr` column is the faulty value from `sirt::btm`, while `ssr_alt` comes from computing the correct value as G^2 / (1+G^2).

```{r}
meta_analysis_data <- meta_analysis_data %>% 
  mutate(ssr_alt = sepG^2 / (1+sepG^2))

ssr_comparison_data <- meta_analysis_data %>% 
  transmute(judging_session, ssr, sepG, ssr_alt, ssr_diff = abs(ssr_alt - ssr)) %>% 
  arrange(-ssr_diff)

ssr_comparison_data %>% basic_kable()
```



```{r include=FALSE}
CperR <- round(cj_sessions$observed_N_C/cj_sessions$observed_N_R,0)
meta_analysis_data$CperR <- CperR
meta_analysis_data_34 <- meta_analysis_data[meta_analysis_data$CperR > 33,]
meta_analysis_data_20 <- meta_analysis_data[meta_analysis_data$CperR > 19,]
meta_analysis_data_10 <- meta_analysis_data[meta_analysis_data$CperR > 9,]
```


```{r}
ssr_comparison_data %>% 
  ggplot(aes(x = ssr_alt, y = ssr)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0) +
  labs(x = "Correct SSR", y = "sirt::btm value for SSR")
```

So for the following analyses, we use the correct value of the SSR instead.

```{r}
meta_analysis_data <- meta_analysis_data %>% 
  # keep both values around just in case
  mutate(ssr_bad = ssr) %>% 
  mutate(ssr_good = ssr_alt) %>% 
  # but pick the correct one for going forward
  mutate(ssr = ssr_good) %>% 
  # tidy up the presentation of the adaptivity field
  mutate(adaptivity = str_replace(adaptivity, "TRUE", "Adaptive") %>% str_replace("FALSE", "Non-adaptive"))
```


# SSR vs split-halves

```{r ssr-vs-splithalves}
SSRvSplitHalves <- cor.test(meta_analysis_data$ssr, meta_analysis_data$mean_split_corr)
SSRvSplitHalves_r <- round(SSRvSplitHalves$estimate, 3)
SSRvSplitHalves_p <- round(SSRvSplitHalves$p.value, 3)
meta_analysis_data %>% 
  filter(!is.na(ssr)) %>% 
  mutate(pt_label = ifelse(mean_split_corr < 0.5 | ssr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = ssr, y = mean_split_corr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(SSRvSplitHalves_r), ", p = ", toString(SSRvSplitHalves_p)), 
       x = "SSR", y = "Mean split-halves correlation")
ggsave("figs-pdf/FIG-ssr-vs-splithalves.pdf", units = "cm", width = 14, height = 14)
```


```{r ssr-vs-splithalves-by-adaptivity}
meta_analysis_data %>% 
  filter(!is.na(ssr)) %>% 
  ggplot(aes(x = ssr, y = mean_split_corr)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  ggpubr::stat_cor(p.accuracy = 0.001) +
  facet_grid(cols = vars(adaptivity)) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(x = "SSR", y = "Mean split-halves correlation")
```

### Using SSR of the halves

When forming split halves, we fit the Bradley-Terry model using data from each half of the judges. Since this is a subset of the data, the SSR for the split halves will almost certainly be lower than the SSR for the full dataset.

One way to interpret reliability is "how likely would I be to get results like these if I repeated the process?". The split-halves approach answers this by comparing the scores from two subsets of the judges, A and B. If we imagine instead that group A was the only one that we collected data from, then the SSR from judge group A would be another way to estimate the "reliability", so should bear some relation to the correlation of group A's scores with group B's (imagining group B as "repeating the process" with a new group of judges).

So here, rather than using the SSR of the full dataset on the x-axis, we use only the SSR of one of the split halves. Plotted here is every one of the 100 splits from each of the studies in the sample:

```{r}
if(file.exists("data-cache/split_halves_summary.csv")) {
  splits_stats <- read_csv("data-cache/split_halves_summary.csv", show_col_types = FALSE)
} else {
  files <- list.files(pattern = "split_halves.csv", recursive = TRUE)
  
  splits <- vroom::vroom("data-cache/Bisson2019/split_halves.csv") %>% 
    mutate(data = purrr::map(path, ~ vroom::vroom(.x) %>% select(contains("theta"))))
  
  splits <- tibble(file = list.files(pattern = "split_halves.csv", recursive = TRUE)) %>% 
    #head(n = 10) %>% 
    mutate(file_contents = purrr::map(file, ~ vroom::vroom(.x, show_col_types = FALSE))) %>% 
    unnest(file_contents) %>% 
    mutate(data = purrr::map(path, ~ vroom::vroom(.x, show_col_types = FALSE) %>% select(contains("theta"))))
  
  splits_stats <- splits %>% 
    mutate(
      halves_info = purrr::map(data,
                               function(df) {
                                 df %>%
                                   summarise(
                                     G_x = sd(theta.x) / sqrt(mean(se.theta.x ^ 2)),
                                     ssr_x = G_x ^ 2 / (1 + G_x ^ 2),
                                     G_y = sd(theta.y) / sqrt(mean(se.theta.y ^ 2)),
                                     ssr_y = G_y ^ 2 / (1 + G_y ^ 2)
                                   )
                               })
    ) %>% 
    unnest(cols = c(halves_info))
  
  splits_stats %>% write_csv("data-cache/split_halves_summary.csv")
}



splits_stats %>% 
  ggplot(aes(x = ssr_x, y = split_corr)) +
  geom_point() +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  ggpubr::stat_cor(p.accuracy = 0.001)
```

Here we omit studies where the number of comparisons is too low for the split half to meet the recommendations set out by Verhavert et al., and add the prediction interval to the plot:

```{r}
splits_stats %>% 
  separate(file, into = c(NA, "study", NA), sep = "/") %>% 
  left_join(meta_analysis_data, by = c("study" = "judging_session")) %>% 
  # restrict to those studies that gathered enough data for each split half to meet the Verhavert recommended N_CR
  # note: results are quite different if you do/don't use this filter!
  filter(observed_N_C/observed_N_R > 2*17) %>% 
  # add the prediction interval
  cbind(
    predict(lm(split_corr ~ ssr_x, data = .), interval = "prediction")
  ) %>% 
  ggplot(aes(x = ssr_x, y = split_corr)) +
  geom_point(aes(colour = study), alpha = 0.3) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), fill = "blue", alpha = 0.2) +
  ggpubr::stat_cor(p.accuracy = 0.001) +
  theme(legend.position = "none")
```

This is then a bit like the plot by Verhavert et al. (2018), appearing in the bottom middle (or bottom right) panel of their Figure C1, and similarly shows a fairly "y=x" trend line:

```{r}
splits_stats %>% 
  separate(file, into = c(NA, "study", NA), sep = "/") %>% 
  left_join(meta_analysis_data, by = c("study" = "judging_session")) %>% 
  # restrict to those studies that gathered enough data for each split half to meet the Verhavert recommended N_CR
  # note: results are quite different if you do/don't use this filter!
  filter(observed_N_C/observed_N_R > 2*17) %>% 
  # these points are the outliers:
  # filter(!str_detect(study, "Spehar")) %>% 
  group_by(study) %>% 
  #summarise(ssr_x = mean(ssr_x), split_corr = mean(split_corr)) %>% 
  #should this not be the median?
  summarise(ssr_x = median(ssr_x), split_corr = median(split_corr)) %>% 
  # add the prediction interval
  cbind(
    predict(lm(split_corr ~ ssr_x, data = .), interval = "prediction")
  ) %>% 
  ggplot(aes(x = ssr_x, y = split_corr)) +
  geom_point(alpha = 0.9) +
  #ggrepel::geom_text_repel(aes(label = study), alpha = 0.9) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), fill = "blue", alpha = 0.2) +
  ggpubr::stat_cor(p.accuracy = 0.001) +
  geom_abline(slope = 1) +
  expand_limits(x = 0, y = 0) +
  theme(legend.position = "bottom")
```
### Distributions of values for reliability measures
Reliability measures raincloud with SSR based on all judges
``` {r reliabilities_raincloud}
rq1 <- read.csv("data/00-RQ1_table_fewercolumns.csv")
#get rid of summary stats rows
rq1 <- rq1[1:101,]
# add column of thresholds for coloring
th <- data.frame(th10=(rq1$CompPERrep > 9) * 9, 
                 th20=(rq1$CompPERrep > 19) * 19 ,
                 th34=(rq1$CompPERrep > 33) * 33)
rq1$thresholds <- apply(th, 1, max, na.rm=TRUE)

#get rels columns
rels <- rq1[,c(6:8,12)]
rels <- pivot_longer(rels, 
                     cols = c("SSR", "r", "Correct")
                     )
colnames(rels)[2] <- "key"

# Raincloud plot with repeated measurements
plotRels <- rels %>%                 # define dataframe
  ggplot(aes(x = key,       # define x var
             y = value,     # define y var
             colour = thresholds)) + 
  
  #Add individual observations to the plot
  geom_point(
    position = position_jitter(width=.1), # add jitter to the observations
    alpha=.4, # set the size of each dot. alpha adds transparency
      ) + 
  
  scale_colour_manual(
    values = c("#eff3ff", "#bdd7e7", "#6baed6", "#2171b5"),
    breaks = c(0,10,20,33)
    ) +
  
  #y tick labels
  scale_y_continuous( labels = scales::comma) + #, minor_breaks = c(0.2, 0.3, 0.4, 0.6,0.7, 0.8, 0.9)) +
  
    annotate("text", x = 3.3, y = .6, label = "Scale Separation Reliability") +
    annotate("text", x = 2.3, y = .6, label = "Split-halves reliability") +
    annotate("text", x = 1.3, y = .6, label = "Correct comparisons") +
  
  # Optional styling
  coord_flip() +                          # flip x & y coordinates
  xlab(NULL) +        # x-axis label
  ylab(NULL) + # y-axis label
  theme_minimal() +   
  theme(            # make modifications to the theme
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    panel.grid.major.y=element_blank(),   # hide major grid for y axis
    panel.grid.minor.y=element_blank(),   # hide minor grid for y axis
    panel.grid.major.x=element_line(),    # show major grid for x axis
    panel.grid.minor.x=element_line(),   # hide minor grid for x axis
    legend.position="none")


plotRels
ggsave("figs-pdf/FIG-RQ2_reliabilities_raincloud.pdf", units = "cm", width = 20, height = 14)

```


### SSR vs split-halves for `N_CR` => 10 
NMM recommends 5 judgements per item, i.e. N_CR > 9. With SSR based on all assessors' comparisons:

```{r ssr-vs-splithalves-10}
SSRvSplitHalves_10 <- cor.test(meta_analysis_data_10$ssr, meta_analysis_data_10$mean_split_corr)
SSRvSplitHalves_r_10 <- round(SSRvSplitHalves_10$estimate, 3)
SSRvSplitHalves_p_10 <- round(SSRvSplitHalves_10$p.value, 3)
meta_analysis_data_10 %>% 
  filter(!is.na(ssr)) %>% 
  mutate(pt_label = ifelse(mean_split_corr < 0.5 | ssr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = ssr, y = mean_split_corr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(SSRvSplitHalves_r_10), ", p = ", toString(SSRvSplitHalves_p_10)), 
       x = "SSR", y = "Median split-halves correlation")
ggsave("figs-pdf/FIG-ssr-vs-splithalves-10.pdf", units = "cm", width = 14, height = 14)
```

And here N_CR > 9 with SSR based on half of assessors' comparisons:

```{r ssr_x-vs-splithalves-10}
  
  splits_stats_10 = splits_stats %>% 
  separate(file, into = c(NA, "study", NA), sep = "/") %>%
  left_join(meta_analysis_data, by = c("study" = "judging_session")) %>% 
  filter(!str_detect(study, "Spehar")) %>% 
  filter(CperR > 2*9) %>% 
  group_by(study) %>% 
  summarise(ssr_x = median(ssr_x), median_split_corr = median(split_corr)) %>% 
  filter(!is.na(ssr_x)) %>%  as.data.frame
  
  SSR_xvSplitHalves_10 <- cor.test(splits_stats_10$ssr_x,
                                  splits_stats_10$median_split_corr)
  SSR_xvSplitHalves_r_10 <- round(SSR_xvSplitHalves_10$estimate, 3)
  SSR_xvSplitHalves_p_10 <- round(SSR_xvSplitHalves_10$p.value, 3)

  splits_stats_10 %>% 
  mutate(pt_label = ifelse(median_split_corr < 0.5 | ssr_x < 0.7, study, "")) %>% 
  ggplot(aes(x = ssr_x, y = median_split_corr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(SSR_xvSplitHalves_r_10), ", p = ", toString(SSR_xvSplitHalves_p_10)), 
       x = "SSR", y = "Median split-halves correlation")
ggsave("figs-pdf/FIG-ssr_x-vs-splithalves-10.pdf", units = "cm", width = 14, height = 14)
```


### SSR vs split-halves for `N_CR` >= 20 
Jones recommends 10 judgements per item, i.e. N_CR > 19 with SSR based on all assessors' comparisons:

```{r ssr-vs-splithalves-20}
SSRvSplitHalves_20 <- cor.test(meta_analysis_data_20$ssr, meta_analysis_data_20$mean_split_corr)
SSRvSplitHalves_r_20 <- round(SSRvSplitHalves_20$estimate, 3)
SSRvSplitHalves_p_20 <- round(SSRvSplitHalves_20$p.value, 3)
meta_analysis_data_20 %>% 
  filter(!is.na(ssr)) %>% 
  mutate(pt_label = ifelse(mean_split_corr < 0.5 | ssr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = ssr, y = mean_split_corr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(SSRvSplitHalves_r_20), ", p = ", toString(SSRvSplitHalves_p_20)), 
       x = "SSR", y = "Median split-halves correlation")
ggsave("figs-pdf/FIG-ssr-vs-splithalves-20.pdf", units = "cm", width = 14, height = 14)
```
And here N_CR > 19 with SSR based on half of assessors' comparisons:

```{r ssr_x-vs-splithalves-20}
  
  splits_stats_20 = splits_stats %>% 
  separate(file, into = c(NA, "study", NA), sep = "/") %>% 
  left_join(meta_analysis_data, by = c("study" = "judging_session")) %>% 
  filter(!str_detect(study, "Spehar")) %>% 
  filter(CperR > 2*19) %>% 
  group_by(study) %>% 
  summarise(ssr_x = median(ssr_x), median_split_corr = median(split_corr)) %>% 
  filter(!is.na(ssr_x)) %>%  as.data.frame
  
  SSR_xvSplitHalves_20 <- cor.test(splits_stats_20$ssr_x,
                                  splits_stats_20$median_split_corr)
  SSR_xvSplitHalves_r_20 <- round(SSR_xvSplitHalves_20$estimate, 3)
  SSR_xvSplitHalves_p_20 <- round(SSR_xvSplitHalves_20$p.value, 3)

  splits_stats_20 %>% 
  mutate(pt_label = ifelse(median_split_corr < 0.5 | ssr_x < 0.7, study, "")) %>% 
  ggplot(aes(x = ssr_x, y = median_split_corr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(SSR_xvSplitHalves_r_20), ", p = ", toString(SSR_xvSplitHalves_p_20)), 
       x = "SSR", y = "Median split-halves correlation")
ggsave("figs-pdf/FIG-ssr_x-vs-splithalves-20.pdf", units = "cm", width = 14, height = 14)
```



### SSR vs split-halves for `N_CR` >= 34 
Verhavert et al. 2019 recommend 17 judgements per item, i.e. N_CR > 33  with SSR based on all assessors' comparisons:

```{r ssr-vs-splithalves-34}
SSRvSplitHalves_34 <- cor.test(meta_analysis_data_34$ssr, meta_analysis_data_34$mean_split_corr)
SSRvSplitHalves_r_34 <- round(SSRvSplitHalves_34$estimate, 3)
SSRvSplitHalves_p_34 <- round(SSRvSplitHalves_34$p.value, 3)
meta_analysis_data_34 %>% 
  filter(!is.na(ssr)) %>% 
  mutate(pt_label = ifelse(mean_split_corr < 0.5 | ssr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = ssr, y = mean_split_corr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(SSRvSplitHalves_r_34), ", p = ", toString(SSRvSplitHalves_p_34)), 
       x = "SSR", y = "Mean split-halves correlation")
ggsave("figs-pdf/FIG-ssr-vs-splithalves-34.pdf", units = "cm", width = 14, height = 14)
```
And here N_CR > 33 with SSR based on half of assessors' comparisons:

```{r ssr_x-vs-splithalves-34}
  
  splits_stats_34 = splits_stats %>% 
  separate(file, into = c(NA, "study", NA), sep = "/") %>% 
  left_join(meta_analysis_data, by = c("study" = "judging_session")) %>% 
  filter(!str_detect(study, "Spehar")) %>% 
  filter(CperR > 2*33) %>% 
  group_by(study) %>% 
  summarise(ssr_x = median(ssr_x), median_split_corr = median(split_corr)) %>% 
  filter(!is.na(ssr_x)) %>%  as.data.frame
  
  SSR_xvSplitHalves_34 <- cor.test(splits_stats_34$ssr_x,
                                  splits_stats_34$median_split_corr)
  SSR_xvSplitHalves_r_34 <- round(SSR_xvSplitHalves_34$estimate, 3)
  SSR_xvSplitHalves_p_34 <- round(SSR_xvSplitHalves_34$p.value, 3)

  splits_stats_34 %>% 
  mutate(pt_label = ifelse(median_split_corr < 0.5 | ssr_x < 0.7, study, "")) %>% 
  ggplot(aes(x = ssr_x, y = median_split_corr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(SSR_xvSplitHalves_r_34), ", p = ", toString(SSR_xvSplitHalves_p_34)), 
       x = "SSR", y = "Median split-halves correlation")
ggsave("figs-pdf/FIG-ssr_x-vs-splithalves-34.pdf", units = "cm", width = 14, height = 14)
```


It is very rare for the split-halves measure to be higher than SSR:

```{r}
meta_analysis_data %>% 
  filter(mean_split_corr > ssr) %>% select(judging_session, starts_with("observed"), mean_split_corr, ssr) %>% 
  basic_kable(digits = 3)
```

## Low mean split-halves reliability

These sessions have split-halves reliability below 0.7.

The `N_CR` column shows the "number of comparisons per representation", i.e. `N_C/N_R`.

```{r}
low_splithalves <- meta_analysis_data %>% 
  filter(mean_split_corr < 0.7) %>% 
  select(judging_session, starts_with("observed"), mean_split_corr, ssr) %>% 
  purrr::set_names(~ str_remove(., "observed_")) %>% 
  mutate(N_CR = N_C/N_R, .after = "N_C") %>% 
  arrange(mean_split_corr)

low_splithalves %>% 
  basic_kable(digits = 3)
```

Omitting the three sessions with N_CR = 400:

```{r low-splithaves}
low_splithalves %>% 
  ggplot(aes(y = mean_split_corr, x = N_CR)) +
  geom_point() +
  xlim(c(0,100))
```

# EloChoice

Here we look at how the EloChoice results compare with the Bradley-Terry ones.

## Reliability

First of all, how does the measure of reliability in EloChoice compare with SSR/split-halves?

There are two reliability measures proposed for EloChoice: see discussion in https://cran.r-project.org/web/packages/EloChoice/vignettes/EloChoice-tutorial.html

We use the weighted version, $R'$, computed based on 1000 iterations.

### SSR vs EloChoice

```{r ssr-vs-elo}
SSRvElo <- cor.test(meta_analysis_data$ssr, meta_analysis_data$mean_eloR_weighted, use="complete.obs")
SSRvElo_r <- round(SSRvElo$estimate, 3)
SSRvElo_p <- round(SSRvElo$p.value, 3)
meta_analysis_data %>% 
  filter(!is.na(mean_eloR_weighted)) %>% 
  mutate(pt_label = ifelse(mean_eloR_weighted < 0.5 | ssr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = ssr, y = mean_eloR_weighted, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_point() +
  geom_smooth(method = "lm", formula = "y ~ x") +
  ggrepel::geom_text_repel(alpha = 0.3) +
  geom_abline(slope = 1, intercept = 0) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(SSRvElo_r), ", p = ", toString(SSRvElo_p)), 
       x = "SSR", y = "Mean EloChoice R (weighted)")
```

### Split-halves vs EloChoice

```{r splithalves-vs-elo}
ElovSplitHalves <- cor.test(meta_analysis_data$mean_eloR_weighted, meta_analysis_data$mean_split_corr)
ElovSplitHalves_r <- round(ElovSplitHalves$estimate, 3)
ElovSplitHalves_p <- round(ElovSplitHalves$p.value, 3)
meta_analysis_data %>% 
  filter(!is.na(mean_eloR_weighted)) %>% 
  mutate(pt_label = ifelse(mean_eloR_weighted < 0.5 | mean_split_corr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = mean_split_corr, y = mean_eloR_weighted, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_point() +
  geom_smooth(method = "lm", formula = "y ~ x") +
  ggrepel::geom_text_repel(alpha = 0.3) +
  geom_abline(slope = 1, intercept = 0) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(ElovSplitHalves_r), ", p = ", toString(ElovSplitHalves_p)),
       x = "Mean split-halves", y = "Mean EloChoice R (weighted)")
```

### Split-halves vs EloChoice `N_CR` >= 10 

```{r splithalves-vs-elo-10}
ElovSplitHalves_10 <- cor.test(meta_analysis_data_10$mean_eloR_weighted, meta_analysis_data_10$mean_split_corr)
ElovSplitHalves_r_10 <- round(ElovSplitHalves_10$estimate, 3)
ElovSplitHalves_p_10 <- round(ElovSplitHalves_10$p.value, 3)
meta_analysis_data_10 %>% 
  filter(!is.na(mean_eloR_weighted)) %>% 
  mutate(pt_label = ifelse(mean_eloR_weighted < 0.5 | mean_split_corr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = mean_split_corr, y = mean_eloR_weighted, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_point() +
  geom_smooth(method = "lm", formula = "y ~ x") +
  ggrepel::geom_text_repel(alpha = 0.3) +
  geom_abline(slope = 1, intercept = 0) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(ElovSplitHalves_r_10), ", p = ", toString(ElovSplitHalves_p_10)),
       x = "Mean split-halves", y = "Mean EloChoice R (weighted)")
```

### Split-halves vs EloChoice `N_CR` >= 20 

```{r splithalves-vs-elo-20}
ElovSplitHalves_20 <- cor.test(meta_analysis_data_20$mean_eloR_weighted, meta_analysis_data_20$mean_split_corr)
ElovSplitHalves_r_20 <- round(ElovSplitHalves_20$estimate, 3)
ElovSplitHalves_p_20 <- round(ElovSplitHalves_20$p.value, 3)
meta_analysis_data_20 %>% 
  filter(!is.na(mean_eloR_weighted)) %>% 
  mutate(pt_label = ifelse(mean_eloR_weighted < 0.5 | mean_split_corr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = mean_split_corr, y = mean_eloR_weighted, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_point() +
  geom_smooth(method = "lm", formula = "y ~ x") +
  ggrepel::geom_text_repel(alpha = 0.3) +
  geom_abline(slope = 1, intercept = 0) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(ElovSplitHalves_r_20), ", p = ", toString(ElovSplitHalves_p_20)),
       x = "Mean split-halves", y = "Mean EloChoice R (weighted)")
```

### Split-halves vs EloChoice `N_CR` >= 34 

```{r splithalves-vs-elo-34}
ElovSplitHalves_34 <- cor.test(meta_analysis_data_34$mean_eloR_weighted, meta_analysis_data_34$mean_split_corr)
ElovSplitHalves_r_34 <- round(ElovSplitHalves_34$estimate, 3)
ElovSplitHalves_p_34 <- round(ElovSplitHalves_34$p.value, 3)
meta_analysis_data_34 %>% 
  filter(!is.na(mean_eloR_weighted)) %>% 
  mutate(pt_label = ifelse(mean_eloR_weighted < 0.5 | mean_split_corr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = mean_split_corr, y = mean_eloR_weighted, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_point() +
  geom_smooth(method = "lm", formula = "y ~ x") +
  ggrepel::geom_text_repel(alpha = 0.3) +
  geom_abline(slope = 1, intercept = 0) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(ElovSplitHalves_r_34), ", p = ", toString(ElovSplitHalves_p_34)),
       x = "Mean split-halves", y = "Mean EloChoice R (weighted)")
```

## Scores

How do the scores produced by EloChoice compare with those from Bradley-Terry?

Previous work has found high agreement:

* 0.95 and 0.94 in Clark et al. (2018) https://doi.org/10.1371/journal.pone.0190393
* Kendall's tau score of 0.96 for rank orders in Gray et al. (2022) https://doi.org/10.48550/arXiv.2204.01805

```{r elobtmcorr-histogram}
meta_analysis_data %>% 
  filter(!is.na(elo_btm_correlation)) %>% 
  ggplot(aes(x = elo_btm_correlation)) +
  geom_histogram(binwidth = 0.01)
```

```{r}
meta_analysis_data %>% 
  filter(elo_btm_correlation < 0.9) %>% 
  arrange(-elo_btm_correlation) %>% 
  select(elo_btm_correlation, judging_session, starts_with("observed_"), mean_eloR_weighted, mean_split_corr, ssr) %>% 
  basic_kable()
```

The $R'$ measure is quite low for all of those.

Looking at the relationship of $R'$ with the Elo-BTM correlation:

```{r elo-reliability-vs-btmcorr}
meta_analysis_data %>% 
  filter(!is.na(elo_btm_correlation)) %>% 
  ggplot(aes(x = mean_eloR_weighted, y = elo_btm_correlation)) +
  geom_point()
```

It's odd that there are some judging sessions with low $R'$ but high correlation of Elo and BTM scores.

```{r}
meta_analysis_data %>% 
  filter(elo_btm_correlation > 0.95, mean_eloR_weighted < 0.6) %>% 
  arrange(-elo_btm_correlation) %>% 
  select(elo_btm_correlation, judging_session, starts_with("observed_"), mean_eloR_weighted, mean_split_corr, ssr) %>% 
  basic_kable()
```

Those ones all seem to have a small number of representations - so perhaps the $R'$ measure is systematically underestimating reliability in those cases.


# Proportion of correct judgements

The proportion of judgements that are consistent with the final rank order relate more strongly to SSR and Elo than to split-halves reliability.

```{r}
prop_correct_model <- lm(prop_correct_judgements ~ ssr + mean_split_corr + mean_eloR, data = reliability_stats)
summary(prop_correct_model)
```

## Proportion of correction judgements vs SSR
```{r prop-vs-ssr}
PropvsSSR <- cor.test(meta_analysis_data$prop_correct_judgements, meta_analysis_data$ssr, use = "complete.obs")
PropvsSSR_r <- round(PropvsSSR$estimate, 3)
PropvsSSR_p <- round(PropvsSSR$p.value, 3)
meta_analysis_data %>% 
  filter(!is.na(prop_correct_judgements)) %>% 
  mutate(pt_label = ifelse(prop_correct_judgements < 0.5 | ssr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = prop_correct_judgements, y = ssr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(PropvsSSR_r), ", p = ", toString(PropvsSSR_p)), 
       x = "Proportion of correction judgements", y = "SSR")
ggsave("figs-pdf/FIG-prop-vs-ssr.pdf", units = "cm", width = 14, height = 14)
```

## Proportion of correction judgements vs Elo
```{r prop-vs-elo}
PropvsElo <- cor.test(meta_analysis_data$prop_correct_judgements, meta_analysis_data$mean_eloR_weighted, use = "complete.obs")
PropvsElo_r <- round(PropvsElo$estimate, 3)
PropvsElo_p <- round(PropvsElo$p.value, 3)
meta_analysis_data %>% 
  filter(!is.na(prop_correct_judgements)) %>% 
  mutate(pt_label = ifelse(prop_correct_judgements < 0.5 | ssr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = prop_correct_judgements, y = mean_eloR_weighted, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(PropvsElo_r), ", p = ", toString(PropvsElo_p)), 
       x = "Proportion of correction judgements", y = "Elo")
ggsave("figs-pdf/FIG-prop-vs-elo.pdf", units = "cm", width = 14, height = 14)
```

## Proportion of correction judgements vs split-halves

```{r prop-vs-splithalves}
PropvsSplithalves <- cor.test(meta_analysis_data$prop_correct_judgements, meta_analysis_data$mean_split_corr, use = "complete.obs")
PropvsSplithalves_r <- round(PropvsSplithalves$estimate, 3)
PropvsSplithalves_p <- round(PropvsSplithalves$p.value, 3)
meta_analysis_data %>% 
  filter(!is.na(prop_correct_judgements)) %>% 
  mutate(pt_label = ifelse(prop_correct_judgements < 0.5 | ssr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = prop_correct_judgements, y = mean_split_corr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(PropvsSplithalves_r), ", p = ", toString(PropvsSplithalves_p)), 
       x = "Proportion of correction judgements", y = "Split halves")
ggsave("figs-pdf/FIG-prop-vs-splithalves.pdf", units = "cm", width = 14, height = 14)
```

### Proportion of correction judgements vs split-halves for `N_CR` >= 10 

```{r prop-vs-splithalves-10}
PropvsSplithalves_10 <- cor.test(meta_analysis_data_10$prop_correct_judgements, meta_analysis_data_10$mean_split_corr, use = "complete.obs")
PropvsSplithalves_r_10 <- round(PropvsSplithalves_10$estimate, 3)
PropvsSplithalves_p_10 <- round(PropvsSplithalves_10$p.value, 3)
meta_analysis_data_10 %>% 
  filter(!is.na(prop_correct_judgements)) %>% 
  mutate(pt_label = ifelse(prop_correct_judgements < 0.5 | ssr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = prop_correct_judgements, y = mean_split_corr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(PropvsSplithalves_r_10), ", p = ", toString(PropvsSplithalves_p_10)), 
       x = "Proportion of correction judgements", y = "Split halves")
ggsave("figs-pdf/FIG-prop-vs-splithalves-10.pdf", units = "cm", width = 14, height = 14)
```

### Proportion of correction judgements vs split-halves for `N_CR` >= 20 

```{r prop-vs-splithalves-20}
PropvsSplithalves_20 <- cor.test(meta_analysis_data_20$prop_correct_judgements, meta_analysis_data_20$mean_split_corr, use = "complete.obs")
PropvsSplithalves_r_20 <- round(PropvsSplithalves_20$estimate, 3)
PropvsSplithalves_p_20 <- round(PropvsSplithalves_20$p.value, 3)
meta_analysis_data_20 %>% 
  filter(!is.na(prop_correct_judgements)) %>% 
  mutate(pt_label = ifelse(prop_correct_judgements < 0.5 | ssr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = prop_correct_judgements, y = mean_split_corr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(PropvsSplithalves_r_20), ", p = ", toString(PropvsSplithalves_p_20)), 
       x = "Proportion of correction judgements", y = "Split halves")
ggsave("figs-pdf/FIG-prop-vs-splithalves-20.pdf", units = "cm", width = 14, height = 14)
```

### Proportion of correction judgements vs split-halves for `N_CR` >= 34 

```{r prop-vs-splithalves-34}
PropvsSplithalves_34 <- cor.test(meta_analysis_data_34$prop_correct_judgements, meta_analysis_data_34$mean_split_corr, use = "complete.obs")
PropvsSplithalves_r_34 <- round(PropvsSplithalves_34$estimate, 3)
PropvsSplithalves_p_34 <- round(PropvsSplithalves_34$p.value, 3)
meta_analysis_data_34 %>% 
  filter(!is.na(prop_correct_judgements)) %>% 
  mutate(pt_label = ifelse(prop_correct_judgements < 0.5 | ssr < 0.7, judging_session, "")) %>% 
  ggplot(aes(x = prop_correct_judgements, y = mean_split_corr, label = pt_label)) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "#CC0000") +
  geom_abline(slope = 1, intercept = 0, alpha = 0.5) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  #ggrepel::geom_text_repel(alpha = 0.3) +
  xlim(c(0,1)) +
  ylim(c(0,1)) +
  labs(title = paste("r = ", toString(PropvsSplithalves_r_34), ", p = ", toString(PropvsSplithalves_p_34)), 
       x = "Proportion of correction judgements", y = "Split halves")
ggsave("figs-pdf/FIG-prop-vs-splithalves-34.pdf", units = "cm", width = 14, height = 14)
```
