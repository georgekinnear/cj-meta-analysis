---
title: "CJ meta-analysis: final screening results"
author: "George Kinnear"
date: "28/06/2022"
always_allow_html: true
output:
  github_document: default
  html_document: default
---

```{r setup, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(echo = FALSE)

# Tables
library(knitr)
library(kableExtra)
basic_kable = function(df, ...) {
  df %>% 
    kable(...) %>%
    kable_styling(bootstrap_options = "striped", full_width = F)
}
```

Here we review the results from the "final screening" spreadsheet.

```{r message=FALSE, warning=FALSE}
# load the archived copy created by the 05-archive-final-screening-xlsx.R script
all_papers_in_screening <- read_csv("2022-06-07-all-papers.csv")
```


# Literature search summary

Our steps are:

## 1. Lens search (N=1538)

We had various .bib files from different variants of the search (see [01-lens-topup](https://github.com/georgekinnear/cj-meta-analysis/blob/main/lit-search/01-lens-topup.md)).

These were consolidated into `20210702 CJMetaAnalysis.bib`, where the decisions about which ones to carry forward were recorded.

In [03-final-screening](https://github.com/georgekinnear/cj-meta-analysis/blob/main/lit-search/03-final-screening.md), we summarised the contents of the .bib file and produced the "final screening" spreadsheet.

HOWEVER, the .bib file included some entries with publication dates before our cut-off of 2010. We manually deleted these from the final screening spreadsheet at the time.

Here we filter the .bib contents to focus only on those published in 2010 or later, and show the breakdown of the decisions:

```{r}
full_bib <- bib2df::bib2df("../20210702 CJMetaAnalysis.bib") %>% 
  mutate(
    decision = case_when(
      str_detect(GROUPS, "Not in English") ~ "Exclude - Not in English",
      str_detect(GROUPS, "Not Relevant") ~ "Exclude - Not Relevant",
      str_detect(GROUPS, "Theoretical") ~ "Exclude - Theoretical",
      str_detect(GROUPS, "Request") ~ "Request data",
      TRUE ~ NA_character_
    )
  )

lens_search_results <- full_bib %>% 
  # remove the JabRef metadata entries
  filter(!str_detect(BIBTEXKEY, "jabref-meta")) %>% 
  mutate(YEAR = parse_number(YEAR)) %>% 
  filter(YEAR >= 2010)

lens_search_results %>% 
  janitor::tabyl(decision) %>% 
  janitor::adorn_totals() %>% 
  janitor::adorn_pct_formatting(,,,,"percent") %>% 
  basic_kable()
```


## 2. Filtered based on abstracts (N=205)

According to the above results, 200 papers were put into the final screening spreadsheet.

In fact, a further 5 were added to the spreadsheet as a result of reliability checks on the first pass of screening based on abstracts (i.e., Ben had rejected these on the first pass, but when George/Ian were doing reliability checks on 10% of the rejected ones, these were flagged as being worth including after all).

```{r}
all_papers_in_screening %>% 
  filter(bibtexkey %in% c(
"lens.org/009-314-826-753-228",
"lens.org/020-900-024-180-456",
"lens.org/071-419-270-496-367",
"lens.org/183-375-439-686-983",
"lens.org/116-204-409-837-73X"
)) %>% 
  basic_kable()
```

Here is the info about how they were coded originally:

```{r}
full_bib %>% 
  filter(BIBTEXKEY %in% c(
"lens.org/009-314-826-753-228",
"lens.org/020-900-024-180-456",
"lens.org/071-419-270-496-367",
"lens.org/183-375-439-686-983",
"lens.org/116-204-409-837-73X"
)) %>% 
  basic_kable()
```

They were all originally labelled "Not relevant", so from that category which originally had 1134 in it, we should deduct these 5, leaving 1129 in the "Not relevant category.

## 3. Identified as using CJ, authors contacted for data (N=102)

```{r}
all_papers_in_screening %>% 
  filter(request_data == TRUE, source == "Lens search") %>% 
  janitor::tabyl(category) %>% 
  janitor::adorn_totals() %>% 
  janitor::adorn_pct_formatting(,,,,"percent") %>% 
  basic_kable()
```

## 4. Data retrieved (N=34+5)

From the 102 studies where data was requested, we received the data for 34:

```{r}
all_papers_in_screening %>% 
  filter(data_retrieved == TRUE, source == "Lens search") %>% 
  janitor::tabyl(category) %>% 
  janitor::adorn_totals() %>% 
  janitor::adorn_pct_formatting(,,,,"percent") %>% 
  basic_kable()
```

In addition, we received data through snowballing, for a further 5 studies:

```{r}
all_papers_in_screening %>% 
  filter(data_retrieved == TRUE, source == "Snowballing") %>% 
  janitor::tabyl(category) %>% 
  janitor::adorn_totals() %>% 
  janitor::adorn_pct_formatting(,,,,"percent") %>% 
  basic_kable()
```


See here for example of creating a flowchart using DiagrammeR:

https://mikeyharper.uk/flowcharts-in-r-using-diagrammer/

```
# Define some sample data
data <- list(a=1000, b=800, c=600, d=400)


DiagrammeR::grViz("
digraph graph2 {

graph [layout = dot]

# node definitions with substituted label text
node [shape = rectangle, width = 4, fillcolor = Biege]
a [label = '@@1']
b [label = '@@2']
c [label = '@@3']
d [label = '@@4']

a -> b -> c -> d

}

[1]:  paste0('Raw Data (n = ', data$a, ')')
[2]: paste0('Remove Errors (n = ', data$b, ')')
[3]: paste0('Identify Potential Customers (n = ', data$c, ')')
[4]: paste0('Select Top Priorities (n = ', data$d, ')')
")
```
