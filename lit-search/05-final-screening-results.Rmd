---
title: "CJ meta-analysis: final screening results"
author: "George Kinnear"
date: "28/06/2022"
always_allow_html: true
output:
  html_document: default
  github_document: default
---

```{r setup, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(echo = FALSE)

# Tables
library(knitr)
library(kableExtra)
basic_kable = function(df, ...) {
  df %>% 
    kable(...) %>%
    kable_styling(bootstrap_options = "striped", full_width = F)
}
```

Here we review the results from the "final screening" spreadsheet.

```{r message=FALSE, warning=FALSE}
# load the archived copy created by the 05-archive-final-screening-xlsx.R script
all_papers_in_screening <- read_csv("2022-06-07-all-papers.csv")
```


# Literature search summary

Our steps are:

## 1. Lens search (N=1538)

We had various .bib files from different variants of the search (see [01-lens-topup](https://github.com/georgekinnear/cj-meta-analysis/blob/main/lit-search/01-lens-topup.md)).

These were consolidated into `20210702 CJMetaAnalysis.bib`, where the decisions about which ones to carry forward were recorded.

In [03-final-screening](https://github.com/georgekinnear/cj-meta-analysis/blob/main/lit-search/03-final-screening.md), we summarised the contents of the .bib file and produced the "final screening" spreadsheet.

HOWEVER, the .bib file included some entries with publication dates before our cut-off of 2010. We manually deleted these from the final screening spreadsheet at the time.

Here we filter the .bib contents to focus only on those published in 2010 or later, and show the breakdown of the decisions:

```{r message=FALSE, warning=FALSE}
full_bib <- bib2df::bib2df("../20210702 CJMetaAnalysis.bib") %>% 
  mutate(
    decision = case_when(
      str_detect(GROUPS, "Not in English") ~ "Exclude - Not in English",
      str_detect(GROUPS, "Not Relevant") ~ "Exclude - Not Relevant",
      str_detect(GROUPS, "Theoretical") ~ "Exclude - Theoretical",
      str_detect(GROUPS, "Request") ~ "Request data",
      TRUE ~ NA_character_
    )
  )

lens_search_results <- full_bib %>% 
  # remove the JabRef metadata entries
  filter(!str_detect(BIBTEXKEY, "jabref-meta")) %>% 
  mutate(YEAR = parse_number(YEAR)) %>% 
  filter(YEAR >= 2010)

lens_search_results %>% 
  janitor::tabyl(decision) %>% 
  janitor::adorn_totals() %>% 
  janitor::adorn_pct_formatting(,,,,"percent") %>% 
  basic_kable()
```


## 2. Filtered based on abstracts (N=205)

According to the above results, 200 papers were put into the final screening spreadsheet.

In fact, a further 5 were added to the spreadsheet as a result of reliability checks on the first pass of screening based on abstracts (i.e., Ben had rejected these on the first pass, but when George/Ian were doing reliability checks on 10% of the rejected ones, these were flagged as being worth including after all).

```{r}
all_papers_in_screening %>% 
  filter(bibtexkey %in% c(
"lens.org/009-314-826-753-228",
"lens.org/020-900-024-180-456",
"lens.org/071-419-270-496-367",
"lens.org/183-375-439-686-983",
"lens.org/116-204-409-837-73X"
)) %>% 
  basic_kable()
```

Here is the info about how they were coded originally:

```{r}
full_bib %>% 
  filter(BIBTEXKEY %in% c(
"lens.org/009-314-826-753-228",
"lens.org/020-900-024-180-456",
"lens.org/071-419-270-496-367",
"lens.org/183-375-439-686-983",
"lens.org/116-204-409-837-73X"
)) %>% 
  basic_kable()
```

They were all originally labelled "Not relevant", so from that category which originally had 1134 in it, we should deduct these 5, leaving 1129 in the "Not relevant category.

## 3. Identified as using CJ, authors contacted for data (N=102)

```{r}
all_papers_in_screening %>% 
  filter(request_data == TRUE, source == "Lens search") %>% 
  janitor::tabyl(category) %>% 
  janitor::adorn_totals() %>% 
  janitor::adorn_pct_formatting(,,,,"percent") %>% 
  basic_kable()
```

## 4. Data retrieved (N=33+5)

From the 102 studies where data was requested, we received the data for 33:

```{r}
all_papers_in_screening %>% 
  filter(data_retrieved == TRUE, source == "Lens search") %>% 
  janitor::tabyl(category) %>% 
  janitor::adorn_totals() %>% 
  janitor::adorn_pct_formatting(,,,,"percent") %>% 
  basic_kable()
```

In addition, we received data through snowballing, for a further 5 studies:

```{r}
all_papers_in_screening %>% 
  filter(data_retrieved == TRUE, source == "Snowballing") %>% 
  janitor::tabyl(category) %>% 
  janitor::adorn_totals() %>% 
  janitor::adorn_pct_formatting(,,,,"percent") %>% 
  basic_kable()
```

```{r}
# Define some sample data
data <- list(
  step1 = nrow(lens_search_results),
  step2 = all_papers_in_screening %>% filter(source == "Lens search") %>% nrow(),
  step3 = all_papers_in_screening %>%
    filter(request_data == TRUE, source == "Lens search") %>% nrow(),
  step4 = all_papers_in_screening %>%
    filter(data_retrieved == TRUE, source == "Lens search") %>% nrow(),
  snowball = all_papers_in_screening %>%
    filter(data_retrieved == TRUE, source == "Snowballing") %>% nrow(),
  final = all_papers_in_screening %>%
    filter(data_retrieved == TRUE) %>% nrow()
)

step2outdata <- lens_search_results %>% 
  janitor::tabyl(decision) %>% 
  janitor::adorn_totals() %>% 
  filter(str_detect(decision, "Exclude")) %>% 
  # fix the 5 entries that were added back in after reliability check
  mutate(n = ifelse(decision == "Exclude - Not Relevant", n - 5, n)) %>% 
  transmute(text = str_glue("{str_remove(decision, 'Exclude - ')} (n={n})")) %>%
  pull(text) %>% 
  paste(collapse = "\n")

flowchart <- DiagrammeR::grViz("
digraph graph2 {

graph [layout = dot]

# node definitions with substituted label text
node [shape = rectangle, width = 4, fillcolor = Biege]
step1 [label = '@@1', fontname = 'Arial']
step2 [label = '@@2', fontname = 'Arial']
step3 [label = '@@3', fontname = 'Arial']
step4 [label = '@@4', fontname = 'Arial']
step5 [label = '@@5', fontname = 'Arial']
step2out [label = '@@6', fontname = 'Arial']
step3out [label = '@@7', fontname = 'Arial']
snowball [label = '@@8', fontname = 'Arial']
final [label = '@@9', fontname = 'Arial']

# make the exclusions appear on the same level as their step
subgraph { rank = same; step2; step2out }
subgraph { rank = same; step3; step3out }
subgraph { rank = same; step5; snowball }

step1 -> step2 -> step3 -> step4 -> step5
step2 -> step2out
step3 -> step3out
snowball -> final
step5 -> final

}

[1]: paste0('Records identified from lens.org (n = ', data$step1, ')')
[2]: paste0('Records screened (n = ', data$step1, ')')
[3]: paste0('Reports assessed for eligibility (n = ', data$step2, ')')
[4]: paste0('Reports identified as eligible (n = ', data$step3, ')')
[5]: paste0('Data obtained (n = ', data$step4, ')')
[6]: paste0('Records excluded: \\n ', step2outdata)
[7]: paste0('Reports excluded (n = 103)')
[8]: paste0('Additional reports identified (n = ', data$snowball, ')')
[8]: paste0('Final dataset (n = ', data$final, ')')
")

flowchart %>% 
  DiagrammeRsvg::export_svg() %>% 
  writeLines("flowchart.svg")

rsvg::rsvg_pdf(svg = "flowchart.svg", file = "flowchart.pdf")

flowchart
```
