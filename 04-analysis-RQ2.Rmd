---
title: 'CJ meta-analysis: RQ2'
author: "George Kinnear"
date: "2023-08-15"
output: github_document
always_allow_html: true
---

```{r setup, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.path='figs-web/04-analysis-RQ2/')
knitr::opts_chunk$set(dpi=300,fig.width=7)

# for plotting
theme_set(theme_minimal())

library(knitr)
library(kableExtra)
basic_kable = function(df, ...) {
  df %>% 
    kable(...) %>%
    kable_styling(bootstrap_options = "striped", full_width = F)
}
```

# About the sample

```{r}
cj_sessions <- read_csv("data/00-judging_sessions_summary.csv", show_col_types = FALSE)
reliability_stats <- read_csv("data/01-meta-analysis-data.csv", show_col_types = FALSE)

meta_analysis_data <- cj_sessions %>% 
  left_join(reliability_stats, by = "judging_session")

meta_analysis_data %>% 
  summarise(n_datatsets = n_distinct(judging_session))
```

# Observed distribution of reliability measures

For these plots:

* Scale Separation Reliability (SSR) is the value of SSR computed from the item scores and standard errors using all of the available judgement data,
* Split-halves reliability comes from computing the Pearson correlation coefficient of the scores produced by fitting the Bradley-Terry model separately on two randomly-selected partitions of the judges, and taking the median of 100 such random splits,
* Correct comparisons is the proportion of individual decisions that agree with the final rank order of the items.

```{r}
observed_reliabilities <- meta_analysis_data %>% 
  select(judging_session, ssr, median_split_corr, prop_correct_judgements) %>% 
  pivot_longer(-judging_session, names_to = "measure", values_to = "value") %>% 
  mutate(measure = case_when(
    measure == "ssr" ~ "Scale Separation Reliability",
    measure == "median_split_corr" ~ "Split-halves reliability",
    measure == "prop_correct_judgements" ~ "Correct comparisons"
  )) %>% 
  mutate(measure = fct_relevel(measure, "Scale Separation Reliability", "Split-halves reliability", "Correct comparisons"))

plot_reliabilities <- observed_reliabilities %>% 
  ggplot(aes(x = measure, y = value)) +
  #Add individual observations to the plot
  geom_point(
    position = position_jitter(width=.3, seed = 123), # add jitter to the observations, using seed so it's reproducible
    alpha=.4, # set the size of each dot. alpha adds transparency
    color = "black", fill = "black") +
  geom_boxplot(width = 0.1, position = position_nudge(x = -0.5)) +
  #scale_y_continuous(trans='log10', labels = scales::comma, minor_breaks = rep(1:9, 21)*(10^rep(-10:10, each=9))) +
  facet_wrap(~ measure, scales = "free_y", ncol = 1) + 
  coord_flip() +
  labs(x = "", y = "") +
  theme_minimal(base_size = 12) +
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    # left align the headings
    strip.text.x = element_text(hjust = 0),
    # add space between the rows
    panel.spacing = unit(1, "lines"),
  )


plot_reliabilities +
  # add max/min/median: https://stackoverflow.com/a/55762626
  stat_summary(
    geom = "text",
    fun = quantile,
    fun.args = list(probs = c(0, 0.5, 1)), # don't need the 0.25 and 0.75!
    aes(label = sprintf("%1.02f", after_stat(y))),
    position = position_nudge(x = -0.8),
    size = 3,
    colour = "#999999"
  ) +
  expand_limits(x = -0.1) # stop the boxplot labels getting chopped off

ggsave("figs-pdf/FIG_RQ2-distribution.pdf", units = "cm", width = 15, height = 9)
```


## Relationship between observed measures

```{r, scatter, fig.height=6, fig.width=6}
plot_scatter <- observed_reliabilities %>%
  left_join(observed_reliabilities, by = "judging_session", relationship = "many-to-many") %>%
  filter(measure.x != measure.y) %>%
  filter(measure.x != "Correct comparisons") %>%
  filter(measure.y != "Scale Separation Reliability") %>%
  ggplot(aes(x = value.x, y = value.y)) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", formula = "y ~ x", colour = "#003399", se = FALSE) +
  geom_abline(slope = 1, intercept = 0) +
  facet_grid(
    rows = vars(measure.y),
    cols = vars(measure.x),
    scales = "free",
    switch = "both"
  ) +
  labs(x = "", y = "") +
  lims(x = c(0,1), y = c(0,1)) +
  theme_minimal(base_size = 12) +
  theme(
    aspect.ratio = 1,
    # put the facet titles outside the y-axis ticks
    strip.placement = "outside",
    # add space between the panels
    panel.spacing = unit(2, "lines"),
  )

# kill off the unused top right panel! https://stackoverflow.com/a/49525552
grob <- ggplotGrob(plot_scatter);
# Remove facet
grob$grobs[[which(grob$layout$name %in% c("panel-1-2"))]] <- grid::nullGrob();
# Plot
grid::grid.newpage()
grid::grid.draw(grob)

ggsave("figs-pdf/FIG_RQ2-observed-scatterplot.pdf", plot = grob, units = "cm", width = 14, height = 14)
```


# Effect of N_CR on reliability measures

> TODO - decide which thresholds to use

```{r}
reliabilities_ncr <- observed_reliabilities %>% 
  left_join(cj_sessions %>% select(judging_session, observed_N_C, observed_N_R), join_by(judging_session)) %>% 
  mutate(N_CR = observed_N_C / observed_N_R, .keep = "unused")

reliabilities_ncr_plot_data <-
  bind_rows(
    "all" = reliabilities_ncr,
    "5+" = reliabilities_ncr %>% filter(N_CR >= 5),
    "10+" = reliabilities_ncr %>% filter(N_CR >= 10),
    "17+" = reliabilities_ncr %>% filter(N_CR >= 17),
    .id = "ncr"
  ) %>% 
  mutate(ncr = fct_relevel(ncr, "all", "5+", "10+", "17+"))

reliabilities_ncr_plot_data %>% 
  ggplot(aes(x = ncr, y = value)) +
  geom_point(
    position = position_jitter(width=.1, seed = 123), # add jitter to the observations, using seed so it's reproducible
    alpha=.2, # set the size of each dot. alpha adds transparency
    color = "black", fill = "black", size = 0.4) +
  geom_boxplot(width = 0.2, position = position_nudge(x = 0.3), linewidth = 0.3, outlier.size = 0.4) +
  facet_grid(cols = vars(measure)) +
  labs(y = "", x = "Number of comparisons per representation")

ggsave("figs-pdf/FIG_RQ2-varying-ncr.pdf", units = "cm", width = 14, height = 7)
```


```{r}
reliabilities_ncr <- observed_reliabilities %>% 
  left_join(cj_sessions %>% select(judging_session, observed_N_C, observed_N_R), join_by(judging_session)) %>% 
  mutate(N_CR = observed_N_C / observed_N_R, .keep = "unused")

reliabilities_ncr_plot_data <-
  bind_rows(
    "all" = reliabilities_ncr,
    "10+" = reliabilities_ncr %>% filter(N_CR >= 10),
    "20+" = reliabilities_ncr %>% filter(N_CR >= 20),
    "34+" = reliabilities_ncr %>% filter(N_CR >= 34),
    .id = "ncr"
  ) %>% 
  mutate(ncr = fct_relevel(ncr, "all", "10+", "20+", "34+")) %>% 
  mutate(highlight = if_else(ncr == "all", "highlight", "plain"))

reliabilities_ncr_plot_data %>% 
  ggplot(aes(x = ncr, y = value, color = highlight)) +
  scale_colour_manual(
    values = c("highlight" = "#08519C", "plain" = "#777777"),
    aesthetics = c("colour", "fill")
  ) +
  geom_point(
    aes(color = highlight, fill = highlight),
    position = position_jitter(width = 0.1, seed = 123),
    # add jitter to the observations, using seed so it's reproducible
    alpha = 0.2,
    size = 0.4
  ) +
  geom_boxplot(
    width = 0.5,
    varwidth = TRUE,
    position = position_nudge(x = 0.45),
    linewidth = 0.3,
    outlier.size = 0.4
  ) +
  facet_grid(cols = vars(measure)) +
  labs(y = "", x = "Number of comparisons per representation") +
  theme(
    legend.position = "none",
    axis.title.x = element_text(size = 9)
  )

ggsave("figs-pdf/FIG_RQ2-varying-ncr.pdf", units = "cm", width = 14, height = 7)
```

```{r}
reliabilities_ncr_plot_data %>% 
  summarise(
    min = min(value),
    median = median(value),
    max = max(value),
    .by = c(ncr, measure)
  ) %>% 
  basic_kable(digits = 3)
```



# Relationships between reliability measures

The split-halves reliability comes from the median of 100 random iterations of the split-halves correlation coefficient.

For each iteration, the judges are split randomly into two groups, and Bradley-Terry is run for each half of the data separately.

Thus, we can consider the SSR of either of those halves.

When running all the split-half computations, we recorded for each split: the value of the correlation (`split_corr`) and the value of the SSR of one of the halves (`ssr_x`):

```{r}
raw_sims <- read_csv("data-cache/split_halves_summary_stats.csv", show_col_types = FALSE)

# take a peek
raw_sims %>% arrange(judging_session, iteration) %>% head()
```

Now the question is: how do `ssr_x` and `split_corr` relate? (That would give some information towards the situation where you have an SSR value for some judging data, and would like to know the likely correlation you would get between the scores generated from that data, and the scores from another similar group of judges.)

From each judging session, we have 100 data points to address that question, but they are not really independent since they are built on the same judgement data. So we summarise each judging session by the *median* of `ssr_x` and `split_corr` across the 100 different splits. We could have used the *mean* but it doesn't actually make much difference as an estimate of the expected value; both averages are very similar in practice, as shown in this plot of the raw values and their averages (mean in red, median in green) in the first 16 sets of judging data:

```{r}
raw_sims_head <- raw_sims %>% 
  group_by(judging_session) %>% 
  filter(cur_group_id() <= 16) %>% 
  ungroup()

raw_sims_head %>% 
  ggplot(aes(x = ssr_x, y = split_corr, group = judging_session)) +
  geom_point() +
  geom_point(data = raw_sims_head %>% summarise(ssr_x = mean(ssr_x), split_corr = mean(split_corr), .by = "judging_session"), colour = "red", shape = 24, size = 3) +
  geom_point(data = raw_sims_head %>% summarise(ssr_x = median(ssr_x), split_corr = median(split_corr), .by = "judging_session"), colour = "green", shape = 25, size = 3) +
  facet_wrap(~ judging_session)
```

So using the medians, we have the following picture:

```{r}
ssr_vs_splithalves_plot1 <- meta_analysis_data %>% 
  ggplot(aes(x = median_ssr_x, y = median_split_corr)) +
  geom_abline(slope = 1, intercept = 0, alpha = 0.3) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_point(alpha = 0.7) +
  geom_smooth(method = "lm", formula = "y ~ x", se = FALSE, colour = "#003399") +
  labs(x = "Median SSR of half the judges", y = "Median split-halves correlation") +
  lims(x = c(0.25,1), y = c(0.25,1)) +
  theme_minimal(base_size = 12) +
  theme(
    aspect.ratio = 1,
  )

ssr_vs_splithalves_plot1
```

We can add a 95% prediction interval from the linear regression:

```{r}
pred_interval <- tibble(median_ssr_x = seq(0.5, 1, by = 0.01))
prediction_interval_data <- bind_cols(
  pred_interval, predict(lm(formula = median_split_corr ~ median_ssr_x, data = meta_analysis_data), new = pred_interval, se.fit = TRUE, interval = "prediction")$fit
  )

ssr_vs_splithalves_plot1 +
  geom_line(data = prediction_interval_data, aes(x = median_ssr_x, y = lwr), colour = "#00339999") +
  geom_line(data = prediction_interval_data, aes(x = median_ssr_x, y = upr), colour = "#00339999")
```

So for instance, with an SSR of 0.8 we might then expect to get a correlation of anywhere between 0.55 and 0.9 with scores generated by a similar group of judges.


## Considering the effect of N_CR

We split the data into groups, according to which ones meet various thresholds that have been recommended in the past for N_CR, the number of comparisons per representation.

```{r}
reliability_corr_plot_data <- meta_analysis_data %>% 
  mutate(N_CR = observed_N_C / observed_N_R) %>% 
  select(judging_session, median_ssr_x, median_split_corr, N_CR) %>%
  mutate(ncr = case_when(
    N_CR >= 34 ~ "34+",
    N_CR >= 20 ~ "20+",
    N_CR >= 10 ~ "10+",
    .default = "0+"
  ))
```

The trend seems to be that with increasing N_CR, the relationship between the two measures gets closer to the y=x line. (i.e., with sufficient judgements that split-halves makes sense, the SSR is  split-halves reliability)

```{r}
ncr_thresholds_colours <- c(
  "0+" = RColorBrewer::brewer.pal(6, "Blues")[3],
  "10+" = RColorBrewer::brewer.pal(6, "Blues")[4],
  "20+" = RColorBrewer::brewer.pal(6, "Blues")[5],
  "34+" = RColorBrewer::brewer.pal(6, "Blues")[6]
)
reliability_corr_plot_data %>% 
  ggplot(aes(x = median_ssr_x, y = median_split_corr, colour = ncr)) +
  geom_abline(slope = 1, intercept = 0, alpha = 0.3) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_point(aes(shape = ncr), alpha = 0.7) +
  scale_colour_manual(values = ncr_thresholds_colours, aesthetics = c("colour", "fill")) +
  scale_shape_manual(values = c(18:15)) +
  #geom_smooth(method = "lm", formula = "y ~ x", se = FALSE) +
  geom_smooth(data = reliability_corr_plot_data %>% filter(N_CR > 0), colour = ncr_thresholds_colours["0+"], method = "lm", formula = "y ~ x", se = FALSE) +
  geom_smooth(data = reliability_corr_plot_data %>% filter(N_CR >= 10), colour = ncr_thresholds_colours["10+"], method = "lm", formula = "y ~ x", se = FALSE) +
  geom_smooth(data = reliability_corr_plot_data %>% filter(N_CR >= 20), colour = ncr_thresholds_colours["20+"], method = "lm", formula = "y ~ x", se = FALSE) +
  geom_smooth(data = reliability_corr_plot_data %>% filter(N_CR >= 34), colour = ncr_thresholds_colours["34+"], method = "lm", formula = "y ~ x", se = FALSE) +
  labs(
    x = "Median SSR of half the judges",
    y = "Median split-halves correlation",
    shape = "Comparisons per representation",
    colour = "Comparisons per representation"
  ) +
  lims(x = c(0.5,1), y = c(0.25,1)) +
  theme_minimal(base_size = 12) +
  theme(
    aspect.ratio = 1,
    legend.position = "bottom"
  )

ggsave("figs-pdf/FIG_RQ2-ssr-vs-splithalves.pdf", units = "cm", width = 14, height = 14)
```

```{r}
ncr_thresholds_colours <- c(
  "0+" = RColorBrewer::brewer.pal(6, "Blues")[3],
  "10+" = RColorBrewer::brewer.pal(6, "Blues")[4],
  "20+" = RColorBrewer::brewer.pal(6, "Blues")[5],
  "34+" = RColorBrewer::brewer.pal(6, "Blues")[6]
)
tibble(ncr_threshold = c(0, 10, 20, 34)) %>%  
  mutate(relevant_data = map(ncr_threshold, ~ reliability_corr_plot_data %>% filter(N_CR >= .x))) %>% 
  unnest(relevant_data) %>% 
  ggplot(aes(x = median_ssr_x, y = median_split_corr, group = ncr_threshold)) +
  geom_abline(slope = 1, intercept = 0, alpha = 0.3) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_point(aes(colour = ncr), alpha = 0.7) +
  scale_colour_manual("Comparisons per representation", values = ncr_thresholds_colours, aesthetics = c("colour", "fill")) +
  geom_smooth(method = "lm", formula = "y ~ x", se = FALSE) +
  labs(x = "Median SSR of half the judges", y = "Median split-halves correlation") +
  lims(x = c(0.5,1), y = c(0.25,1)) +
  theme_minimal(base_size = 12) +
  theme(
    aspect.ratio = 1,
    legend.position = "top"
  ) +
  facet_wrap(~ ncr_threshold)
```

However, looking at the correlations somewhat undermines that interpretation, as the correlation between the two variables decreases as the threshold on N_CR is increased:

```{r}
reliability_corr_values <- tibble(ncr_threshold = c(0, 10, 20, 34)) %>%  
  mutate(relevant_data = map(ncr_threshold, ~ reliability_corr_plot_data %>% filter(N_CR >= .x))) %>% 
  mutate(test_results = map(relevant_data, ~ cor.test(.x$median_ssr_x, .x$median_split_corr))) %>% 
  mutate(
    num_studies = map_dbl(relevant_data, nrow),
    correlation = map_dbl(test_results, ~ .x$estimate),
    p_value = map_dbl(test_results, ~ .x$p.value),
  ) %>% 
  select(ncr_threshold, num_studies, correlation, p_value)

reliability_corr_values %>% 
  basic_kable(digits = 3)
```

(This seems to be driven by a couple of low outliers in the 34+ group.)

```{r}
reliability_corr_plot_data %>% 
  filter(N_CR >= 34, median_ssr_x > 0.8, median_split_corr < 0.7, )
```


Likewise, the details for the linear model in each case show that the variance explained decreases with higher thresholds, i.e. `median_ssr_x` is less predictive of `median_split_corr`:

```{r}
summary(lm(formula = median_split_corr ~ median_ssr_x, data = reliability_corr_plot_data))
summary(lm(formula = median_split_corr ~ median_ssr_x, data = reliability_corr_plot_data %>% filter(N_CR >= 10)))
summary(lm(formula = median_split_corr ~ median_ssr_x, data = reliability_corr_plot_data %>% filter(N_CR >= 20)))
summary(lm(formula = median_split_corr ~ median_ssr_x, data = reliability_corr_plot_data %>% filter(N_CR >= 34)))
```

## Recreating Verhavert et al. (2019) Figure 4

Verhavert et al. (2019) define N_CR to be *double* the ratio N_C/N_R, to account for the fact that each CJ decision involves two items (e.g. 100 decisions about 10 items will mean that each item is present in 100/10*2 = 20 decisions, i.e. has been compared 20 times).

```{r}
verhavert_data <- read_delim("https://zenodo.org/record/2586084/files/Assessment_characterisitcs_data.csv?download=1", delim = ";", locale=locale(decimal_mark = ","), show_col_types = FALSE)
```


```{r}
ssr_vs_ncr <- meta_analysis_data %>%
    mutate(N_CR = observed_N_C / observed_N_R * 2) %>%
    select(judging_session, adaptivity, N_CR, SSR = ssr)

bind_rows(
  "Verhavert" = verhavert_data %>%
    transmute(judging_session = paste(Assessment, `Assessor group`), N_CR, SSR),
  "Kinnear" = ssr_vs_ncr,
  .id = "study"
) %>% 
  filter(N_CR < 60) %>% 
  ggplot(aes(x = N_CR, y = SSR, shape = study, colour = study)) +
  geom_point()
```

We seem to have many more studies with low N_CR yet high SSR.

```{r}
ssr_vs_ncr %>% 
  filter(N_CR < 20, SSR > 0.9) %>% 
  basic_kable()
```

The bulk of those are from Pollitt, where it is likely that adaptivity was used -- and this is known to inflate SSR.

Indeed, the adaptivity seems to produce separate clusters in our sample:

```{r}
bind_rows(
  "Verhavert" = verhavert_data %>%
    transmute(judging_session = paste(Assessment, `Assessor group`), N_CR, SSR),
  "Kinnear (non-adaptive)" = ssr_vs_ncr %>%
    filter(adaptivity == FALSE),
  "Kinnear (adaptive)" = ssr_vs_ncr %>%
    filter(adaptivity == TRUE),
  "Kinnear (unknown)" = ssr_vs_ncr %>%
    filter(adaptivity == "unknown"),
  .id = "study"
) %>% 
  filter(N_CR < 60) %>% 
  ggplot(aes(x = N_CR, y = SSR, shape = study, colour = study)) +
  geom_point()
```

Let's see what the main SSR vs split-halves plot looks like for only the non-adaptive studies.

```{r}
reliability_corr_plot_data_nonadaptive <- reliability_corr_plot_data %>% 
  semi_join(ssr_vs_ncr %>% filter(adaptivity == FALSE), join_by(judging_session))

reliability_corr_plot_data_nonadaptive %>% 
  ggplot(aes(x = median_ssr_x, y = median_split_corr, colour = ncr)) +
  geom_abline(slope = 1, intercept = 0, alpha = 0.3) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_point(aes(shape = ncr), alpha = 0.7) +
  scale_colour_manual(values = ncr_thresholds_colours, aesthetics = c("colour", "fill")) +
  scale_shape_manual(values = c(18:15)) +
  #geom_smooth(method = "lm", formula = "y ~ x", se = FALSE) +
  geom_smooth(data = reliability_corr_plot_data_nonadaptive %>% filter(N_CR > 0), colour = ncr_thresholds_colours["0+"], method = "lm", formula = "y ~ x", se = FALSE) +
  geom_smooth(data = reliability_corr_plot_data_nonadaptive %>% filter(N_CR >= 10), colour = ncr_thresholds_colours["10+"], method = "lm", formula = "y ~ x", se = FALSE) +
  geom_smooth(data = reliability_corr_plot_data_nonadaptive %>% filter(N_CR >= 20), colour = ncr_thresholds_colours["20+"], method = "lm", formula = "y ~ x", se = FALSE) +
  geom_smooth(data = reliability_corr_plot_data_nonadaptive %>% filter(N_CR >= 34), colour = ncr_thresholds_colours["34+"], method = "lm", formula = "y ~ x", se = FALSE) +
  labs(
    x = "Median SSR of half the judges",
    y = "Median split-halves correlation",
    shape = "Comparisons per representation",
    colour = "Comparisons per representation"
  ) +
  lims(x = c(0.5,1), y = c(0.25,1)) +
  theme_minimal(base_size = 12) +
  theme(
    aspect.ratio = 1,
    legend.position = "bottom"
  )
```

Or the simpler version, with adaptive vs non-adaptive highlighted:

```{r}
meta_analysis_data %>% 
  ggplot(aes(x = median_ssr_x, y = median_split_corr, colour = adaptivity, shape = adaptivity)) +
  geom_abline(slope = 1, intercept = 0, alpha = 0.3) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_point(alpha = 0.7) +
  geom_smooth(method = "lm", formula = "y ~ x", se = FALSE) +
  labs(x = "Median SSR of half the judges", y = "Median split-halves correlation") +
  lims(x = c(0.25,1), y = c(0.25,1)) +
  theme_minimal(base_size = 12) +
  theme(
    aspect.ratio = 1,
  )
```

This suggests that the SSR/split-halves relationship looks quite similar whether studies used adaptivity or not.
