---
title: 'CJ meta-analysis: RQ2'
author: "George Kinnear"
date: "2023-08-15"
output: github_document
always_allow_html: true
---

```{r setup, include=FALSE}
library(tidyverse)
library(rstatix)
library(lm.beta)
library(skimr)
library(quantreg)
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.path='figs-web/04-analysis-RQ2/')
knitr::opts_chunk$set(dpi=300,fig.width=7)

# for plotting
theme_set(theme_minimal())

library(knitr)
library(kableExtra)
basic_kable = function(df, ...) {
  df %>% 
    kable(...) %>%
    kable_styling(bootstrap_options = "striped", full_width = F)
}

apa2dp = function(x) {
  #formatC(x, digits = 2, format = "f") %>% str_replace(., "0\\.", ".")
  format(round(x, digits = 2), nsmall = 2) %>% str_replace('0.', '.')
}
apa_pvalue = Vectorize(function(p) {
  if(p < 0.001) {
    "<0.001"
  } else {
    format(round(p, digits = 3), nsmall = 3) %>% str_replace('0.', '.')
  }
})
apa_pstars = Vectorize(function(p) {
  case_when(
      p < 0.001 ~ "***",
      p < 0.01 ~ "**",
      p < 0.05 ~ "*",
      .default = ""
  )
})
```

# About the sample

```{r}
cj_sessions <- read_csv("data/00-judging_sessions_summary.csv", show_col_types = FALSE)
reliability_stats <- read_csv("data/01-meta-analysis-data.csv", show_col_types = FALSE)

meta_analysis_data <- cj_sessions %>% 
  left_join(reliability_stats, by = "judging_session")

meta_analysis_data %>% 
  summarise(n_datatsets = n_distinct(judging_session))
```

## SSR correction

The `ssr` column stores the `mle.rel` value from `sirt::btm` which is incorrect; here, we replace the `ssr` values with the correct ones, computed from `sepG`:

```{r}
meta_analysis_data <- meta_analysis_data %>% 
  mutate(ssr = sepG^2 / (1+sepG^2))
```


# Observed distribution of reliability measures

For these plots:

* Scale Separation Reliability (SSR) is the value of SSR computed from the item scores and standard errors using all of the available judgement data,
* Split-halves reliability comes from computing the Pearson correlation coefficient of the scores produced by fitting the Bradley-Terry model separately on two randomly-selected partitions of the judges, and taking the median of 100 such random splits,
* Correct comparisons is the proportion of individual decisions that agree with the final rank order of the items.

```{r reliabilities-distribution}
observed_reliabilities <- meta_analysis_data %>%
  select(judging_session, ssr, median_split_corr) %>%
  pivot_longer(-judging_session, names_to = "measure", values_to = "value") %>%
  mutate(
    measure = case_when(
      measure == "ssr" ~ "Scale Separation Reliability",
      measure == "median_split_corr" ~ "Split-halves reliability"
    )
  ) %>%
  mutate(measure = fct_relevel(
    measure,
    "Scale Separation Reliability",
    "Split-halves reliability"
  ))

plot_reliabilities <- observed_reliabilities %>%
  ggplot(aes(x = measure, y = value)) +
  #Add individual observations to the plot
  geom_point(
    position = position_jitter(width = .3, seed = 123),
    # add jitter to the observations, using seed so it's reproducible
    alpha = .4,
    # set the size of each dot. alpha adds transparency
    color = "black",
    fill = "black"
  ) +
  geom_boxplot(
    width = 0.1,
    position = position_nudge(x = -0.5),
    outlier.shape = 4
  ) +
  #scale_y_continuous(trans='log10', labels = scales::comma, minor_breaks = rep(1:9, 21)*(10^rep(-10:10, each=9))) +
  facet_wrap( ~ measure, scales = "free_y", ncol = 1) +
  coord_flip() +
  labs(x = "", y = "") +
  theme_minimal(base_size = 12) +
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    # left align the headings
    strip.text.x = element_text(hjust = 0),
    # add space between the rows
    panel.spacing = unit(1, "lines"),
  )


plot_reliabilities +
  # add max/min/median: https://stackoverflow.com/a/55762626
  stat_summary(
    geom = "text",
    fun = quantile,
    fun.args = list(probs = c(0, 0.5, 1)), # don't need the 0.25 and 0.75!
    aes(label = sprintf("%1.02f", after_stat(y))),
    position = position_nudge(x = -0.8),
    size = 3,
    colour = "#999999"
  ) +
  expand_limits(x = -0.1) # stop the boxplot labels getting chopped off

ggsave("figs-pdf/FIG_RQ2-distribution.pdf", units = "cm", width = 15, height = 6)
```



# Effect of N_CR on reliability measures

Verhavert et al. (2019) defined N_CR as 2 * N_C / N_R, with the multiplier of 2 reflecting the fact that each judgement contributes a comparison for each of the items being compared.

With N_CR = 2 * N_C / N_R, we consider various thresholds that have been suggested in the literature:

* *N_CR >= 10*. "assuming 10 decisions per script (or 5 pairs) for a good level of reliability." (Wheadon et al., 2020, p. 59)
* *N_CR >= 20*. “A general rule of thumb is to have at least 10 times the number of judgements to the number of scripts.” (Bisson et al., 2016, p. 154)
* *N_CR >= 37*. “between 10 and 14 comparisons per representation are needed to reach a reliability of .70. To reach a reliability of .90, 26 to 37 comparisons per representation are needed.” (Verhavert et al., 2019, p. 557)

```{r}
reliabilities_ncr <- observed_reliabilities %>% 
  left_join(cj_sessions %>% select(judging_session, observed_N_C, observed_N_R), join_by(judging_session)) %>% 
  mutate(N_CR = 2 * observed_N_C / observed_N_R, .keep = "unused")

reliabilities_ncr_plot_data <-
  bind_rows(
    "all" = reliabilities_ncr,
    "10+" = reliabilities_ncr %>% filter(N_CR >= 10),
    "20+" = reliabilities_ncr %>% filter(N_CR >= 20),
    "37+" = reliabilities_ncr %>% filter(N_CR >= 37),
    .id = "ncr"
  ) %>% 
  mutate(ncr = fct_relevel(ncr, "all", "10+", "20+", "37+")) %>% 
  mutate(highlight = if_else(ncr == "all", "highlight", "plain"))

reliabilities_ncr_plot_data %>%
  group_by(ncr) %>%
  distinct(judging_session) %>%
  tally(name = "num_studies") %>% 
  basic_kable()
```

Since there is only one study that is just below the 10+ threshold, we will just proceed with the all/20+/37+ groups.

```{r}
reliabilities_ncr_plot_data <- reliabilities_ncr_plot_data %>% 
  filter(ncr != "10+")
```

```{r reliabilities-by-ncr, include=FALSE}
reliabilities_ncr_plot_data %>% 
  ggplot(aes(x = ncr, y = value, color = highlight)) +
  scale_colour_manual(
    values = c("highlight" = "#08519C", "plain" = "#777777"),
    aesthetics = c("colour", "fill")
  ) +
  geom_point(
    aes(color = highlight, fill = highlight),
    position = position_jitter(width = 0.1, seed = 123),
    # add jitter to the observations, using seed so it's reproducible
    alpha = 0.2,
    size = 0.4
  ) +
  geom_boxplot(
    width = 0.5,
    varwidth = TRUE,
    position = position_nudge(x = 0.45),
    linewidth = 0.3,
    outlier.size = 0.4
  ) +
  facet_grid(cols = vars(measure)) +
  labs(y = "", x = "Number of comparisons per representation") +
  theme(
    legend.position = "none",
    axis.title.x = element_text(size = 9)
  )

ggsave("figs-pdf/FIG_RQ2-varying-ncr.pdf", units = "cm", width = 14, height = 7)
```

```{r reliabilities-by-ncr-with-shapes}
reliabilities_ncr %>% 
  filter(measure!="Correct comparisons") %>%
  mutate(ncr_shape = case_when(N_CR < 20 ~ 0,
                               N_CR < 37 ~ 1,
                               N_CR < 1000000 ~ 2)) %>%
  ggplot(aes(x = measure, y = value)) + 
  geom_point(
    position = position_jitter(width=.3, seed = 123), # add jitter to the observations, using seed so it's reproducible
    alpha=.4, # set the size of each dot. alpha adds transparency
    aes(shape = factor(ncr_shape))
  ) +
  geom_boxplot(width = 0.1, position = position_nudge(x = -0.5)) +
  facet_wrap(~ measure, scales = "free_y", ncol = 1) + 
  coord_flip() +
  labs(x = "", y = "") +
  theme_minimal(base_size = 12) +
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    # left align the headings
    strip.text.x = element_text(hjust = 0),
    # add space between the rows
    panel.spacing = unit(1, "lines"),
    legend.position = "none"
  ) +
   stat_summary(
    geom = "text",
    group = 1,
    fun = quantile,
    fun.args = list(probs = c(0, 0.5, 1)), # don't need the 0.25 and 0.75!
    aes(label = sprintf("%1.02f", after_stat(y))),
    position = position_nudge(x = -0.8),
    size = 3,
    colour = "#999999"
  ) +
  expand_limits(x = -0.1) # stop the boxplot labels getting chopped off


ggsave("figs-pdf/FIG_RQ2a.pdf", units = "cm", width = 14, height = 7)
```

```{r}
reliabilities_ncr_plot_data %>% 
  summarise(
    min = min(value),
    median = median(value),
    max = max(value),
    `prop_above_0.7`= sum(value > 0.7) / n(),
    .by = c(measure, ncr)
  ) %>% 
  basic_kable(digits = 3)
```

### Adaptivity

The trend in "correct comparisons" seems to be down to the adaptive studies being more likely to have high values on that measure while having low N_CR:

```{r adaptivity-correct-comparisons}
meta_analysis_data %>% 
  mutate(N_CR = 2 * observed_N_C / observed_N_R) %>% 
  ggplot(aes(x = adaptivity, y = prop_correct_judgements)) +
  geom_point(
    position = position_jitter(width = 0.1, seed = 123),
    # add jitter to the observations, using seed so it's reproducible
    alpha = 0.2,
    size = 0.4
  ) +
  geom_boxplot(
    width = 0.5,
    varwidth = TRUE,
    position = position_nudge(x = 0.45),
    linewidth = 0.3,
    outlier.size = 0.4
  ) +
  labs(x = "Adaptivity") +
  theme(
    legend.position = "none",
    axis.title.x = element_text(size = 9)
  )
```

It may also be ascribed to the prevalence of layperson judges in studies with large N_CR:

```{r judge-expertise-ncr}
meta_analysis_data %>% 
  mutate(N_CR = 2 * observed_N_C / observed_N_R) %>% 
  mutate(N_CR_above_37 = if_else(N_CR >= 37, "N_CR >= 37", "N_CR < 37")) %>% 
  ggplot(aes(x = judge_expertise, y = prop_correct_judgements)) +
  geom_point(
    position = position_jitter(width = 0.1, seed = 123),
    # add jitter to the observations, using seed so it's reproducible
    alpha = 0.2,
    size = 0.4
  ) +
  geom_boxplot(
    width = 0.5,
    varwidth = TRUE,
    position = position_nudge(x = 0.45),
    linewidth = 0.3,
    outlier.size = 0.4
  ) +
  labs(y = "Proportion of judgements agreeing with final rank", x = "Judge expertise") +
  theme(
    legend.position = "none",
    axis.title.x = element_text(size = 9)
  ) +
  facet_wrap(~ (N_CR_above_37), ncol = 2)
```

## SSR and split-halves only

Focusing only on the SSR and split-halves measures:

```{r varying-ncr-horiz}
reliabilities_ncr_plot_data %>% 
  filter(measure != "Correct comparisons") %>% 
  ggplot(aes(x = ncr, y = value, color = highlight)) +
  scale_colour_manual(
    values = c("highlight" = "#08519C", "plain" = "#777777"),
    aesthetics = c("colour", "fill")
  ) +
  geom_point(
    aes(color = highlight, fill = highlight),
    position = position_jitter(width = 0.1, seed = 123),
    # add jitter to the observations, using seed so it's reproducible
    alpha = 0.2,
    size = 0.4
  ) +
  geom_boxplot(
    width = 0.5,
    varwidth = TRUE,
    position = position_nudge(x = 0.45),
    linewidth = 0.3,
    outlier.size = 0.4
  ) +
  coord_flip() +
  scale_x_discrete(limits = rev) +
  facet_wrap(~ (measure), ncol = 1) +
  labs(y = "", x = "Comparisons per representation\n") +
  theme(
    legend.position = "none",
    axis.title.y = element_text(size = 9)
  )

ggsave("figs-pdf/FIG_RQ2-varying-ncr-horiz.pdf", units = "cm", width = 14, height = 8)
```


```{r varying-ncr}
reliabilities_ncr_plot_data %>% 
  filter(measure != "Correct comparisons") %>% 
  ggplot(aes(x = ncr, y = value, color = highlight)) +
  scale_colour_manual(
    values = c("highlight" = "#08519C", "plain" = "#777777"),
    aesthetics = c("colour", "fill")
  ) +
  geom_point(
    aes(color = highlight, fill = highlight),
    position = position_jitter(width = 0.1, seed = 123),
    # add jitter to the observations, using seed so it's reproducible
    alpha = 0.2,
    size = 0.4
  ) +
  geom_boxplot(
    width = 0.5,
    varwidth = TRUE,
    position = position_nudge(x = 0.45),
    linewidth = 0.3,
    outlier.size = 0.4
  ) +
  facet_wrap(~ (measure)) +
  labs(y = "", x = "Comparisons per representation\n") +
  theme_minimal(base_size = 12) +
  theme(
    legend.position = "none",
    axis.title.x = element_text(size = 10)
  )

ggsave("figs-pdf/FIG_RQ2-ssr-vs-splithalves-by-ncr.pdf", units = "cm", width = 12, height = 7)
```


# Relationships between reliability measures

## Using SSR_ALL

For each study, we computed the SSR and SHR from the judgement data. Here, each point is one study and we see that the two values are closely related:

```{r ssr-vs-shr}
ssr_vs_shr_data <- meta_analysis_data %>% 
  filter(adaptivity == FALSE)

ssr_all_vs_splithalves_plot <- 
  ssr_vs_shr_data %>% 
  ggplot(aes(x = ssr, y = median_split_corr)) +
  geom_abline(slope = 1, intercept = 0, alpha = 0.3) +
  geom_vline(xintercept = 0.7, alpha = 0.3, linetype = "dashed") +
  geom_hline(yintercept = 0.7, alpha = 0.3, linetype = "dashed") +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", formula = "y ~ x", se = FALSE, 
              colour = "black") +
  #labs(x = expression(SSR[ALL]), y = "Split-halves reliability (SHR)") +
  labs(x = "SSR", y = "Split-halves reliability (SHR)") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = .20)) +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, by = .20)) +
  #lims(x = c(0.25,1), y = c(0.25,1)) +
  theme_minimal(base_size = 12) +
  theme(
    aspect.ratio = 1,
  )

ssr_all_vs_splithalves_plot

ggsave("figs-pdf/FIG_RQ2b-ssrALL-vs-shr.pdf", units = "cm", width = 14, height = 14)
ggsave("figs-pdf/FIG_PANEL_RQ2b-ssrALL-vs-shr.pdf", units = "cm", width = 8, height = 8)

lm(formula = median_split_corr ~ ssr, data = ssr_vs_shr_data) %>% summary()
cor.test(formula = ~ median_split_corr + ssr, data = ssr_vs_shr_data, method = "pearson")
cor.test(formula = ~ median_split_corr + ssr, data = ssr_vs_shr_data, method = "spearman")
```


## Introducing SSR_HALF

The split-halves reliability comes from the median of 100 random iterations of the split-halves correlation coefficient.

For each iteration, the judges are split randomly into two groups, and Bradley-Terry is run for each half of the data separately.

Thus, we can consider the SSR of either of those halves.

When running all the split-half computations, we recorded for each split: the value of the correlation (`split_corr`) and the value of the SSR of one of the halves (`ssr_x`):

```{r}
raw_sims <- read_csv("data-cache/split_halves_summary_stats.csv", show_col_types = FALSE)

# take a peek
raw_sims %>% arrange(judging_session, iteration) %>% head()
```

Now the question is: how do `ssr_x` and `split_corr` relate? (That would give some information towards the situation where you have an SSR value for some judging data, and would like to know the likely correlation you would get between the scores generated from that data, and the scores from another similar group of judges.)

From each judging session, we have 100 data points to address that question, but they are not really independent since they are built on the same judgement data. So we summarise each judging session by the *median* of `ssr_x` and `split_corr` across the 100 different splits. We could have used the *mean* but it doesn't actually make much difference as an estimate of the expected value; both averages are very similar in practice, as shown in this plot of the raw values and their averages (mean in red, median in green) in the first 16 sets of judging data:

```{r raw-sims}
raw_sims_head <- raw_sims %>% 
  group_by(judging_session) %>% 
  filter(cur_group_id() <= 16) %>% 
  ungroup()

raw_sims_head %>% 
  ggplot(aes(x = ssr_x, y = split_corr, group = judging_session)) +
  geom_point(alpha = 0.2) +
  geom_point(data = raw_sims_head %>% summarise(ssr_x = mean(ssr_x), split_corr = mean(split_corr), .by = "judging_session"), colour = "red", shape = 24, size = 3) +
  geom_point(data = raw_sims_head %>% summarise(ssr_x = median(ssr_x), split_corr = median(split_corr), .by = "judging_session"), colour = "green", shape = 25, size = 3) +
  facet_wrap(~ judging_session)
```

So using the medians, we have the following picture:

```{r ssrHALF-vs-shr}
ssr_half_vs_splithalves_plot <- 
  ssr_vs_shr_data %>% 
  ggplot(aes(x = median_ssr_x, y = median_split_corr)) +
  geom_abline(slope = 1, intercept = 0, alpha = 0.3) +
  geom_vline(xintercept = 0.7, alpha = 0.3, linetype = "dashed") +
  geom_hline(yintercept = 0.7, alpha = 0.3, linetype = "dashed") +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", formula = "y ~ x", se = FALSE, 
              colour = "black") +
  labs(x = expression(SSR[HALF]), y = "Split-halves reliability (SHR)") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = .20)) +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, by = .20)) +
  #lims(x = c(0.25,1), y = c(0.25,1)) +
  theme_minimal(base_size = 12) +
  theme(
    aspect.ratio = 1,
  )

ssr_half_vs_splithalves_plot

ggsave("figs-pdf/FIG_RQ2b-ssr-vs-shr.pdf", units = "cm", width = 12, height = 12)
ggsave("figs-pdf/FIG_PANEL_RQ2b-ssrHALF-vs-shr.pdf", units = "cm", width = 8, height = 8)
```
<details><summary>Extra plot</summary>

```{r two-ssr-vs-shr-plots}
library(patchwork)
wrap_plots(ssr_all_vs_splithalves_plot, ssr_half_vs_splithalves_plot) +
  plot_annotation(tag_levels = 'a', tag_prefix = '(', tag_suffix = ')')

ggsave("figs-pdf/FIG_RQ2b-ssr-vs-shr-both-ways.pdf", units = "cm", width = 16, height = 8)
```
</details>


There is a strong linear relationship here:

```{r}
ssr_vs_splithalves_lm <- lm(formula = median_split_corr ~ median_ssr_x, data = ssr_vs_shr_data)

ssr_vs_splithalves_lm %>% summary()
```

We can add a 95% prediction interval from the linear regression:

```{r prediction-interval}
pred_interval <- tibble(median_ssr_x = seq(0.5, 1, by = 0.01))
prediction_interval_data <- bind_cols(
  pred_interval, predict(lm(formula = median_split_corr ~ median_ssr_x, data = ssr_vs_shr_data), new = pred_interval, se.fit = TRUE, interval = "prediction")$fit
  )

ssr_half_vs_splithalves_plot +
  geom_line(data = prediction_interval_data, aes(x = median_ssr_x, y = lwr), colour = "#00339999") +
  geom_line(data = prediction_interval_data, aes(x = median_ssr_x, y = upr), colour = "#00339999")
```

So for instance, with an SSR of 0.8 we might then read off the blue prediction interval and expect to get a correlation of anywhere between 0.5 and 0.9 with scores generated by a similar group of judges.

### Spearman-Brown correction

We apply the Spearman-Brown correction to the SHR value:

\[\text{SRH}_\text{SB}=\frac{2\times\text{SHR}}{1+\text{SHR}}\]

```{r ssr-vs-shr-spearman-brown}
ssr_vs_shr_data %>% 
  mutate(shr_sb = 2*median_split_corr / (1 + median_split_corr)) %>% 
  ggplot(aes(x = ssr, y = shr_sb)) +
  geom_abline(slope = 1, intercept = 0, alpha = 0.3) +
  geom_vline(xintercept = 0.7, alpha = 0.3, linetype = "dashed") +
  geom_hline(yintercept = 0.7, alpha = 0.3, linetype = "dashed") +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", formula = "y ~ x", se = FALSE, 
              colour = "black") +
  labs(x = "SSR", y = "Spearman-Brown corrected SHR") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = .20)) +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, by = .20)) +
  #lims(x = c(0.25,1), y = c(0.25,1)) +
  theme_minimal(base_size = 12) +
  theme(
    aspect.ratio = 1,
  )

ggsave("figs-pdf/FIG_PANEL_RQ2b-ssrALL-vs-shrSB.pdf", units = "cm", width = 8, height = 8)
```


## Comparing SSR and SHR thresholds

If we use a particular SSR threshold in our study, what sort of correlation might we expect with the scores we'd get if we gathered the same number of judgements again?

The above analysis does a sort of simulation of this situation, by splitting the data from each of the studies in the meta-analysis into two halves (which you could think of as the original and replication), and computing:

* SSR_HALF - the SSR of the "original" half dataset,
* SHR - the correlation between the scores from the "original" and "replication" datasets

(where in fact each of these is the median over 100 iterations of splitting the data into random halves).

We can then look at what a given SSR threshold should imply for the SHR.

Here we look at how likely a given SSR threshold is to produce an SHR of .7 or higher:

```{r}
tibble(
  ssr_threshold = c(.7, .75, .8, .85, .9)
) %>% 
  mutate(
    n = purrr::map_int(ssr_threshold, ~ ssr_vs_shr_data %>% filter(median_ssr_x > .x) %>% nrow()),
    n_with_shr_above_pt7 = purrr::map_int(ssr_threshold, ~ ssr_vs_shr_data %>% filter(median_ssr_x > .x) %>% filter(median_split_corr > 0.7) %>% nrow()),
    pct = round(n_with_shr_above_pt7 / n * 100, digits = 0) %>% paste0("%")
  ) %>% 
  basic_kable()
```

Here we look at other SHR thresholds too:

```{r}
crossing(
  ssr_threshold = c(.7, .75, .8, .85, .9),
  shr_threshold = c(.7, .8, .9)
) %>% 
  mutate(
    n = purrr::map_int(ssr_threshold, ~ meta_analysis_data %>% filter(median_ssr_x > .x) %>% nrow()),
    n_good_shr = purrr::map2_int(ssr_threshold, shr_threshold, ~ meta_analysis_data %>% filter(median_ssr_x > .x) %>% filter(median_split_corr > .y) %>% nrow()),
    pct = round(n_good_shr / n * 100, digits = 0) %>% paste0("%"),
    table_entry = str_glue("{n_good_shr} ({pct})")
  ) %>% 
  select(ssr_threshold, shr_threshold, n, table_entry) %>% 
  pivot_wider(names_from = shr_threshold, values_from = table_entry) %>% 
  basic_kable(col.names = c("SSR Threshold", "Number of studies", ".7", ".8", ".9")) %>% 
  add_header_above(c(" " = 2, "SHR Threshold" = 3))
```



## Considering the effect of N_CR

We split the data into groups, according to which ones meet various thresholds that have been recommended in the past for N_CR, the number of comparisons per representation.

```{r}
reliability_corr_plot_data <- meta_analysis_data %>% 
  mutate(N_CR = 2 * observed_N_C / observed_N_R) %>% 
  select(judging_session, median_ssr_x, median_split_corr, N_CR) %>%
  mutate(ncr = case_when(
    N_CR >= 37 ~ "37+",
    N_CR >= 20 ~ "20+",
    #N_CR >= 10 ~ "10+",
    .default = "0+"
  ))
```

The trend seems to be that with increasing N_CR, the relationship between the two measures gets closer to the y=x line. (i.e., with sufficient judgements that split-halves makes sense, the SSR is  split-halves reliability)

```{r ssr-vs-shr-regression}
ncr_thresholds_colours <- c(
  "0+" = RColorBrewer::brewer.pal(6, "Blues")[3],
  #"10+" = RColorBrewer::brewer.pal(6, "Blues")[4],
  "20+" = RColorBrewer::brewer.pal(6, "Blues")[5],
  "37+" = RColorBrewer::brewer.pal(6, "Blues")[6]
)
reliability_corr_plot_data %>% 
  ggplot(aes(x = median_ssr_x, y = median_split_corr, colour = ncr)) +
  geom_abline(slope = 1, intercept = 0, alpha = 0.3) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_point(aes(shape = ncr), alpha = 0.7) +
  scale_colour_manual(values = ncr_thresholds_colours, aesthetics = c("colour", "fill")) +
  scale_shape_manual(values = c(18:15)) +
  #geom_smooth(method = "lm", formula = "y ~ x", se = FALSE) +
  geom_smooth(data = reliability_corr_plot_data %>% filter(N_CR > 0), colour = ncr_thresholds_colours["0+"], method = "lm", formula = "y ~ x", se = FALSE) +
  geom_smooth(data = reliability_corr_plot_data %>% filter(N_CR >= 20), colour = ncr_thresholds_colours["20+"], method = "lm", formula = "y ~ x", se = FALSE) +
  geom_smooth(data = reliability_corr_plot_data %>% filter(N_CR >= 37), colour = ncr_thresholds_colours["37+"], method = "lm", formula = "y ~ x", se = FALSE) +
  labs(
    x = "Median SSR of half the judges",
    y = "Median split-halves correlation",
    shape = "Comparisons per representation",
    colour = "Comparisons per representation"
  ) +
  lims(x = c(0,1), y = c(0,1)) +
  theme_minimal(base_size = 12) +
  theme(
    aspect.ratio = 1,
    legend.position = "bottom"
  )

ggsave("figs-pdf/FIG_RQ2-ssr-vs-splithalves.pdf", units = "cm", width = 14, height = 14)
```

```{r ssr-vs-shr-regression-facets}
tibble(ncr_threshold = c(0, 20, 37)) %>%  
  mutate(relevant_data = map(ncr_threshold, ~ reliability_corr_plot_data %>% filter(N_CR >= .x))) %>% 
  unnest(relevant_data) %>% 
  ggplot(aes(x = median_ssr_x, y = median_split_corr, group = ncr_threshold)) +
  geom_abline(slope = 1, intercept = 0, alpha = 0.3) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_point(aes(colour = ncr), alpha = 0.7) +
  scale_colour_manual("Comparisons per representation", values = ncr_thresholds_colours, aesthetics = c("colour", "fill")) +
  geom_smooth(method = "lm", formula = "y ~ x", se = FALSE) +
  labs(x = "Median SSR of half the judges", y = "Median split-halves correlation") +
  lims(x = c(0.5,1), y = c(0.25,1)) +
  theme_minimal(base_size = 12) +
  theme(
    aspect.ratio = 1,
    legend.position = "top"
  ) +
  facet_wrap(~ ncr_threshold)
```

However, looking at the correlations somewhat undermines that interpretation, as the correlation between the two variables decreases as the threshold on N_CR is increased:

```{r}
reliability_corr_values <- tibble(ncr_threshold = c(0, 20, 37)) %>%  
  mutate(relevant_data = map(ncr_threshold, ~ reliability_corr_plot_data %>% filter(N_CR >= .x))) %>% 
  mutate(test_results = map(relevant_data, ~ cor.test(.x$median_ssr_x, .x$median_split_corr, 
                                                      method = "spearman"))) %>% 
  mutate(
    num_studies = map_dbl(relevant_data, nrow),
    correlation = map_dbl(test_results, ~ .x$estimate),
    p_value = map_dbl(test_results, ~ .x$p.value),
  ) %>% 
  select(ncr_threshold, num_studies, correlation, p_value)

reliability_corr_values %>% 
  basic_kable(digits = 3)
```

(This seems to be driven by a couple of low outliers in the 37+ group.)

```{r}
reliability_corr_plot_data %>% 
  filter(N_CR >= 37, median_ssr_x > 0.8, median_split_corr < 0.7, )
```


Likewise, the details for the linear model in each case show that the variance explained decreases with higher thresholds, i.e. `median_ssr_x` is less predictive of `median_split_corr`:

```{r}
summary(lm(formula = median_split_corr ~ median_ssr_x, data = reliability_corr_plot_data))
summary(lm(formula = median_split_corr ~ median_ssr_x, data = reliability_corr_plot_data %>% filter(N_CR >= 20)))
summary(lm(formula = median_split_corr ~ median_ssr_x, data = reliability_corr_plot_data %>% filter(N_CR >= 37)))
```



# How do study characteristics relate to reliability?

Look at how representations, assessors, comparisons and their derivatives relate to SSR and SHR.

Note that here we restrict to only the *non-adaptive* CJ sessions.

```{r}
characteristics_of_interest <-
  c("observed_N_A",
    "observed_N_R",
    "observed_N_C",
    "N_CA",
    "N_CR",
    "N_RA")

reliability_characteristics <- meta_analysis_data %>%
  mutate(
    N_CA = observed_N_C/observed_N_A,
    N_CR = 2*observed_N_C/observed_N_R,
    N_RA = observed_N_R/observed_N_A) %>% 
  filter(adaptivity == FALSE)

# write out a copy of this data in case it is useful elsewhere
reliability_characteristics %>% 
  select(judging_session, ssr, shr = median_split_corr, all_of(characteristics_of_interest)) %>% 
  rename_with(~ str_remove(.x, "observed_")) %>% 
  mutate(
    log_N_A = log10(N_A),
    log_N_C = log10(N_C),
    log_N_R = log10(N_R)
  ) %>% 
  write.csv("data-cache/RQ2_characteristics_vs_reliability.csv")
```

This table shows the Pearson correlation of each characteristic with SSR and with SHR:

```{r}
rel_corrs <- reliability_characteristics %>%
  cor_test(
    vars = c("median_split_corr", "ssr"),
    vars2 = all_of(characteristics_of_interest),
    method = "pearson"
  ) 

rel_corrs_tab <- rel_corrs %>% 
  select(var1, var2, cor, p) %>% 
  mutate(
    rounded_cor = apa2dp(cor),
    p_stars = case_when(
      p < 0.001 ~ "***",
      p < 0.01 ~ "**",
      p < 0.05 ~ "*",
      .default = ""
    ),
    var2 = str_remove(var2, "observed_"),
    var1 = case_when(
      var1 == "median_split_corr" ~ "SHR",
      var1 == "ssr" ~ "SSR",
      .default = var1
    )
  ) %>% 
  mutate(table_entry = str_glue("{rounded_cor}{p_stars}")) %>% 
  select(var1, var2, table_entry) %>% 
  pivot_wider(names_from = var2, values_from = table_entry)

rel_corrs_tab %>% 
  basic_kable()
```

<details>
  <summary>LaTeX table</summary>
```{r}
rel_corrs_tab %>% basic_kable(format = "latex", booktabs = TRUE) %>% cat()
```
</details>


```{r}
sig_for_shr <- rel_corrs %>% 
  filter(var1 == "median_split_corr") %>% 
  filter(p < 0.05) %>% 
  pull(var2)

sig_for_ssr <- rel_corrs %>% 
  filter(var1 == "ssr") %>% 
  filter(p < 0.05) %>% 
  pull(var2)
```


### Plotting

```{r all-correlations}
library(GGally)

# thanks to https://bookdown.org/content/3686/metric-predicted-variable-with-multiple-metric-predictors.html
show_corr <- function(data, mapping, ...) {
  
  # get the x and y data to use the other code
  x <- eval_data_col(data, mapping$x)
  y <- eval_data_col(data, mapping$y)
  
  # compute the correlations
  corr <- cor.test(x, y, method = "p", use = "pairwise")
  
  # plot the cor value
  ggally_text(
    label = corr$estimate %>% apa2dp(),
    mapping = aes(),
    color = "black",
    size = 3) +
    cowplot::panel_border(size = ifelse(corr$p.value < 0.05, 1, 1/2), color = ifelse(corr$p.value < 0.05, "steelblue4", "grey85"))
}

show_scatter <- function(data, mapping, ...) {
  
  # get the x and y data to use the other code
  x <- eval_data_col(data, mapping$x)
  y <- eval_data_col(data, mapping$y)
  
  # compute the correlations
  corr <- cor.test(x, y, method = "p", use = "pairwise")
  
  ggplot(data = data, mapping = mapping) + 
    #geom_point(size = .75, shape = 21, stroke = 1/10,
    #           color = "white", fill = "steelblue4") +
    geom_point(size = .75, stroke = 0, color = "steelblue4", alpha = 0.3) +
    cowplot::panel_border(size = ifelse(corr$p.value < 0.05, 1, 1/2), color = ifelse(corr$p.value < 0.05, "steelblue4", "grey85"))
}

show_dist <- function(data, mapping, ...) {
  ggplot(data = data, mapping = mapping) + 
    geom_histogram(bins = 30) +
    cowplot::panel_border(size = 1/2)
}

reliability_characteristics %>%
  select(SSR = ssr, SHR = median_split_corr, all_of(characteristics_of_interest)) %>% 
  rename_with(~ str_remove(., "observed_")) %>% 
  ggpairs(
    upper = list(continuous = show_scatter),
    lower = list(continuous = show_corr),
    diag = list(continuous = show_dist),
    progress = FALSE
  ) +
  theme(
    strip.text = element_text(size = 6),
    strip.text.y.right = element_text(angle = 0),
    axis.title.x = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  )

ggsave("figs-pdf/FIG_RQ2_all-correlations.pdf", units = "cm", width = 12, height = 11.5)
```



## Split-halves reliability

Sig correlations with SHR are `r sig_for_shr`. Check scatter plots for SHR.

```{r shr-scatter}
reliability_characteristics %>% 
  select(median_split_corr, all_of(characteristics_of_interest)) %>% 
  pivot_longer(cols = -median_split_corr, names_to = "characteristic") %>% 
  ggplot(aes(x = value, y = median_split_corr)) +
    geom_point() +
    scale_x_continuous(trans = 'log10', labels = scales::comma) +
    facet_wrap(~ characteristic, scales = "free")
```



Forced linear regression SHR:

```{r}
formula_shr <- reformulate(sig_for_shr, response = "median_split_corr")
formula_shr

SHR_lm <- lm(formula = formula_shr,
     data = reliability_characteristics)
summary(SHR_lm)
lm.beta(SHR_lm)
```

```{r}
shr_table <-
  tibble(
    parameter = names(lm.beta(SHR_lm)$standardized.coefficients),
    beta = lm.beta(SHR_lm)$standardized.coefficients,
    p = summary(SHR_lm)$coefficients[, 4]
  ) %>%
  mutate(pvalue = paste0(apa_pvalue(p), apa_pstars(p)), .keep = "unused")
```



## SSR

For SSR we have significant correlation with `r sig_for_ssr`.

```{r ssr-scatter}
reliability_characteristics %>% 
  select(ssr, all_of(characteristics_of_interest)) %>% 
  pivot_longer(cols = -ssr, names_to = "characteristic") %>% 
  ggplot(aes(x = value, y = ssr)) +
    geom_point() +
    scale_x_continuous(trans = 'log10', labels = scales::comma) +
    facet_wrap(~ characteristic, scales = "free")
```

```{r ssr-vs-ncr}
reliability_characteristics %>% 
  select(ssr, N_CR) %>% 
  pivot_longer(cols = -ssr, names_to = "characteristic") %>% 
  ggplot(aes(x = value, y = ssr)) +
    geom_point() +
    scale_x_continuous(trans = 'log10', labels = scales::comma) +
    facet_wrap(~ characteristic, scales = "free")
```


SSR forced linear regression:

```{r}
formula_ssr <- reformulate(sig_for_ssr, response = "ssr")
formula_ssr

SSR_lm <- lm(formula = formula_ssr,
     data = reliability_characteristics)
summary(SSR_lm)
lm.beta(SSR_lm)
```


```{r}
ssr_table <-
  tibble(
    parameter = names(lm.beta(SSR_lm)$standardized.coefficients),
    beta = lm.beta(SSR_lm)$standardized.coefficients,
    p = summary(SSR_lm)$coefficients[, 4]
  ) %>%
  mutate(pvalue = paste0(apa_pvalue(p), apa_pstars(p)), .keep = "unused")
```

```{r}
forced_regressions_table <- shr_table %>% 
  left_join(ssr_table, join_by(parameter), suffix = c("_shr", "_ssr")) %>% 
  mutate(across(starts_with("beta"), ~ round(.x, digits = 2)))

forced_regressions_table %>% 
  basic_kable(col.names = c("Characteristic", "beta", "p", "beta", "p")) %>% 
  add_header_above(c(" " = 1, "SHR" = 2, "SSR" = 2))

```

<details><summary>LaTeX table</summary>

```{r}
forced_regressions_table %>% 
  basic_kable(
    col.names = c("Characteristic", "beta", "p", "beta", "p"),
    format = "latex",
    booktabs = TRUE
  ) %>% 
  add_header_above(c(" " = 1, "SHR" = 2, "SSR" = 2)) %>% 
  cat()
```
</details>

## Regression with interactions

As observed earlier, the three characteristics are inter-related, and correlated predictors pose a problem for regression models. One way of checking for correlated predictors is using the variance inflation factor (VIF), where values above 10 are typically taken as an indicator of high multicollinearity:

```{r}
lm(ssr ~ observed_N_A + observed_N_C + observed_N_R, 
             data = reliability_characteristics) %>% car::vif()
```

Regardless of multicollinearity, we can proceed with a regression model involving all three characteristics, as well as interaction terms between them:

```{r}
lm_chars_interactions <- lm(ssr ~ observed_N_A * observed_N_C * observed_N_R, 
             data = reliability_characteristics)

lm_chars_interactions %>% summary()
```

It is interesting here to see the interaction between $N_C$ and $N_R$ appearing significant in the model.

Focusing on N_CR, here is a model with just $N_C$ and $N_R$ (and their interaction):

```{r}
lm_ncr_interaction <- lm(ssr ~ observed_N_C * observed_N_R, 
             data = reliability_characteristics)

lm_ncr_interaction %>% summary()

lm.beta(lm_ncr_interaction)
```

Another approach would be to consider log-transformed versions of the three characteristics:

```{r}
reliability_characteristics %>% 
  select(ssr, starts_with("observed_")) %>% 
  mutate(across(starts_with("observed"), log10)) %>% 
  rename_with(~ str_replace(.x, "observed_", "log_")) %>% 
  lm(data = ., formula = ssr ~ log_N_A * log_N_C * log_N_R) %>% 
  summary()
```



## What about the suggested thesholds of N_CR>=20 and NC_R>=37?

Let's compare SSR and SHR for these thresholds

```{r}
#select the data we need
thresholds <- reliability_characteristics %>%
  mutate(
    ncr_level = case_when(N_CR < 20 ~ "0. low",
                          N_CR < 37 ~ "1. medium",
                          .default = "2. high")
  )
```

```{r reliability-by-ncr}
thresholds %>% 
  pivot_longer(cols = c(median_split_corr, ssr), names_to = "measure") %>% 
  mutate(
    measure = case_when(
      measure == "ssr" ~ "Scale Separation Reliability",
      measure == "median_split_corr" ~ "Split-halves reliability"
    )
  ) %>% 
  ggplot(aes(x = ncr_level, y = value)) +
  geom_point(
    position = position_jitter(width = 0.1, seed = 123),
    # add jitter to the observations, using seed so it's reproducible
    alpha = 0.2,
    size = 0.4
  ) +
  geom_boxplot(
    width = 0.5,
    varwidth = TRUE,
    position = position_nudge(x = 0.45),
    linewidth = 0.3,
    outlier.size = 0.4
  ) +
  facet_grid(cols = vars(measure)) +
  labs(y = "", x = "Comparisons per representation") +
  scale_x_discrete(
    labels = c("low\n(<20)", "medium\n(20+)", "high\n(37+)")
  ) +
  theme_minimal(base_size = 12) +
  theme(
    legend.position = "none",
    axis.title.x = element_text(size = 9)
  )

ggsave("figs-pdf/FIG_RQ2-ssr-shr-by-ncr.pdf", units = "cm", width = 11, height = 7)
```


For the case of SSR

```{r}
thresholds %>% group_by(ncr_level) %>% 
  summarise(count=n(),
            median = median(ssr),
            min = min(ssr),
            max = max(ssr)) %>% basic_kable()

thresholds %>% kruskal_test(ssr ~ ncr_level) %>% basic_kable()
thresholds %>% dunn_test(ssr ~ ncr_level)  %>% basic_kable()
```



For the case of SHR

```{r}
thresholds %>% group_by(ncr_level) %>% 
  summarise(count=n(),
            median = median(median_split_corr),
            min = min(median_split_corr),
            max = max(median_split_corr)) %>% basic_kable()

thresholds %>% kruskal_test(median_split_corr ~ ncr_level) %>% basic_kable()
thresholds %>% dunn_test(median_split_corr ~ ncr_level) %>% basic_kable()

```


# Recreating Verhavert et al. (2019) Figure 4

Verhavert et al. (2019) analysed data from 49 different CJ sessions.

```{r}
verhavert_data <- read_delim("https://zenodo.org/record/2586084/files/Assessment_characterisitcs_data.csv?download=1", delim = ";", locale=locale(decimal_mark = ","), show_col_types = FALSE)
```

Here we combine their data with our own, to extend their analysis.

```{r combined-with-verhavert}
ssr_vs_ncr <- meta_analysis_data %>%
    mutate(N_CR = 2 * observed_N_C / observed_N_R) %>%
    select(judging_session, adaptivity, N_CR, SSR = ssr)

bind_rows(
  "Verhavert et al." = verhavert_data %>%
    transmute(judging_session = paste(Assessment, `Assessor group`), N_CR, SSR),
  "Kinnear et al." = ssr_vs_ncr,
  .id = "study"
) %>% 
  filter(N_CR < 60) %>% 
  ggplot(aes(x = N_CR, y = SSR, shape = study, colour = study)) +
  geom_point()
```

We seem to have many more studies with low N_CR yet high SSR.

```{r}
ssr_vs_ncr %>% 
  filter(N_CR < 20, SSR > 0.9) %>% 
  basic_kable()
```

The bulk of those are from Pollitt, where it is likely that adaptivity was used -- and this is known to inflate SSR (indeed, Bramley2018_1a was a study designed to show just that).

The adaptivity seems to produce separate clusters in our sample:

```{r ncr-vs-ssr-by-adaptivity}
ncr_vs_ssr_by_adaptivity_data <- bind_rows(
  "Verhavert et al." = verhavert_data %>%
    transmute(judging_session = paste(Assessment, `Assessor group`), N_CR, SSR),
  "non-adaptive" = ssr_vs_ncr %>%
    filter(adaptivity == FALSE),
  "adaptive" = ssr_vs_ncr %>%
    filter(adaptivity == TRUE),
  "unknown" = ssr_vs_ncr %>%
    filter(adaptivity == "unknown"),
  .id = "study"
)

ncr_vs_ssr_by_adaptivity_data %>% 
  filter(N_CR >= 300) %>% 
  basic_kable(caption = "Omitted studies")


ncr_vs_ssr_by_adaptivity_data %>% 
  filter(N_CR >= 100) %>% 
  filter(SSR < 0.8) %>% 
  basic_kable(caption = "Outliers")

ncr_vs_ssr_by_adaptivity_data %>% 
  filter(N_CR < 300) %>% 
  ggplot(aes(x = N_CR, y = SSR, shape = study, colour = study)) +
  geom_point() +
  scale_colour_viridis_d(end = 0.9, option = "plasma") +
  labs(x = expression(N[CR]))

ggsave("figs-pdf/FIG_RQ2-ncr-vs-ssr.pdf", units = "cm", width = 12, height = 8)
```

Let's see what the main SSR vs split-halves plot looks like for only the non-adaptive studies.

```{r ssr-vs-ncr-regression-nonadaptive}
reliability_corr_plot_data_nonadaptive <- reliability_corr_plot_data %>% 
  semi_join(ssr_vs_ncr %>% filter(adaptivity == FALSE), join_by(judging_session))

reliability_corr_plot_data_nonadaptive %>% 
  ggplot(aes(x = median_ssr_x, y = median_split_corr, colour = ncr)) +
  geom_abline(slope = 1, intercept = 0, alpha = 0.3) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_point(aes(shape = ncr), alpha = 0.7) +
  scale_colour_manual(values = ncr_thresholds_colours, aesthetics = c("colour", "fill")) +
  scale_shape_manual(values = c(18:15)) +
  #geom_smooth(method = "lm", formula = "y ~ x", se = FALSE) +
  geom_smooth(data = reliability_corr_plot_data_nonadaptive %>% filter(N_CR > 0), colour = ncr_thresholds_colours["0+"], method = "lm", formula = "y ~ x", se = FALSE) +
  geom_smooth(data = reliability_corr_plot_data_nonadaptive %>% filter(N_CR >= 20), colour = ncr_thresholds_colours["20+"], method = "lm", formula = "y ~ x", se = FALSE) +
  geom_smooth(data = reliability_corr_plot_data_nonadaptive %>% filter(N_CR >= 37), colour = ncr_thresholds_colours["37+"], method = "lm", formula = "y ~ x", se = FALSE) +
  labs(
    x = "Median SSR of half the judges",
    y = "Median split-halves correlation",
    shape = "Comparisons per representation",
    colour = "Comparisons per representation"
  ) +
  lims(x = c(0.5,1), y = c(0.25,1)) +
  theme_minimal(base_size = 12) +
  theme(
    aspect.ratio = 1,
    legend.position = "bottom"
  )
```

Or the simpler version, with adaptive vs non-adaptive highlighted:

```{r ssr-vs-ncr-regression-by-adaptivity}
meta_analysis_data %>% 
  ggplot(aes(x = median_ssr_x, y = median_split_corr, colour = adaptivity, shape = adaptivity)) +
  geom_abline(slope = 1, intercept = 0, alpha = 0.3) +
  geom_vline(xintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_hline(yintercept = 0.7, alpha = 0.3, colour = "red") +
  geom_point(alpha = 0.7) +
  geom_smooth(method = "lm", formula = "y ~ x", se = FALSE) +
  labs(x = "Median SSR of half the judges", y = "Median split-halves correlation") +
  lims(x = c(0.25,1), y = c(0.25,1)) +
  theme_minimal(base_size = 12) +
  theme(
    aspect.ratio = 1,
  )
```

This suggests that the SSR/split-halves relationship looks quite similar whether studies used adaptivity or not.


### Regression with logit link function

Following Verhavert et al. (2019), try using a logit link function on our data:

```{r}
reliability_characteristics %>% 
  glm(ssr ~ N_CR, data = .,
    family = gaussian(link = "logit")) %>% summary()
```

This replicates the finding from Verhavert et al. (2019), that N_CR was significant in the model.


> TODO - the logit link is not supported in `stan_glm`:

```{r}
library(rstanarm)
fit2 <- 
reliability_characteristics %>% 
  filter(adaptivity == FALSE) %>%  
  filter(N_CR < 100) %>% 
  stan_glm(
  ssr ~ 1 + N_CR,
  data = .,
  family = gaussian(link = "log"),
  prior_intercept = normal(0, 10),
  refresh = 0,
  # for speed of example only
  chains = 4, iter = 2000
)

fit2

plot(fit2, plotfun = "areas", prob = 0.9,
     pars = c("(Intercept)", "N_CR"))
```

```{r}
library(tidybayes)
library(modelr)

rel_dat <- reliability_characteristics %>% 
  filter(adaptivity == FALSE) %>% 
  filter(N_CR < 100)
rel_dat %>%
  data_grid(N_CR = seq_range(N_CR, n = 51)) %>%
  add_epred_draws(fit2) %>%
  ggplot(aes(x = N_CR, y = ssr)) +
  stat_lineribbon(aes(y = .epred)) +
  geom_point(data = rel_dat)
```

